{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scoring metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# model selection stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import scipy\n",
    "# scaling and normalisation metrics\n",
    "from sklearn import preprocessing\n",
    "# classifier models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "# graphical stuff\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# feature selection\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "# dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# helper function to print the data\n",
    "def print_results(headline, true_value, pred):\n",
    "    print(headline)\n",
    "    print(\"accuracy: {}\".format(accuracy_score(true_value, pred)))\n",
    "    print(\"precision: {}\".format(precision_score(true_value, pred)))\n",
    "    print(\"recall: {}\".format(recall_score(true_value, pred)))\n",
    "    print(\"f1: {}\".format(f1_score(true_value, pred)))\n",
    "\n",
    "# helper function to build the model\n",
    "def build_lr_model_for_data(data, target):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42, test_size=0.2)\n",
    "    pipeline = make_pipeline(LogisticRegression())\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    return (X_test, y_test, model)\n",
    "\n",
    "# helper function to build the model\n",
    "def build_svc_model_for_data(data, target):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42, test_size=0.2)\n",
    "    pipeline = make_pipeline(SVC())\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    return (X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and remove the feature titles\n",
    "df = pd.read_csv('cc_clients.csv')\n",
    "data = df.drop(df.index[[0]])\n",
    "print('before: {}'.format(data.shape))\n",
    "data.drop_duplicates()\n",
    "print('after: {}'.format(data.shape))\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate the data into features and target\n",
    "training_data = np.asarray(data.loc[:, 'X1':'X23']).astype(np.float)\n",
    "target_data = np.asarray(data['Y']).astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Statistics to see how the dataset is transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results from over/undersampling vs. normal distribution\n",
    "print(\"Normal data distribution: {}\".format(Counter(target_data)))\n",
    "\n",
    "X_nearmiss, y_nearmiss = NearMiss().fit_sample(training_data, target_data)\n",
    "print(\"NearMiss(Undersampling) data distribution: {}\".format(Counter(y_nearmiss)))\n",
    "\n",
    "X_smote, y_smote = SMOTE().fit_sample(training_data, target_data)\n",
    "print(\"SMOTE(Oversampling) data distribution: {}\".format(Counter(y_smote)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare classification models(LR, SVM)\n",
    "classifier_lr = LogisticRegression\n",
    "classifier_svm = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NearMiss undersampling vs SMOTE oversampling(Logistic Regression and Support Vector Machines)<h4> - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normal model\n",
    "pipeline = make_pipeline(classifier_lr(random_state=42))\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nearmiss_pipeline = make_pipeline_imb(NearMiss(random_state=42), classifier_lr(random_state=42))\n",
    "nearmiss_model = nearmiss_pipeline.fit(X_train, y_train)\n",
    "nearmiss_prediction = nearmiss_model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(random_state=42), classifier_lr(random_state=42))\n",
    "smote_model = smote_pipeline.fit(X_train, y_train)\n",
    "smote_prediction = smote_model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing) \n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Score the models using the test data\n",
    "print('Logistic Regression accuracy, no class distribution {}'.format(pipeline.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, NearMiss undersampling {}'.format(nearmiss_pipeline.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, SMOTE oversampling {}'.format(smote_pipeline.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print_results(\"normal classification\", y_test, prediction)\n",
    "print()\n",
    "print_results(\"SMOTE classification\", y_test, smote_prediction)\n",
    "print()\n",
    "print_results(\"NearMiss classification\", y_test, nearmiss_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normal model\n",
    "pipeline = make_pipeline(classifier_svm)\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nearmiss_pipeline = make_pipeline_imb(NearMiss(random_state=42), classifier_svm)\n",
    "nearmiss_model = nearmiss_pipeline.fit(X_train, y_train)\n",
    "nearmiss_prediction = nearmiss_model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model = smote_pipeline.fit(X_train, y_train)\n",
    "smote_prediction = smote_model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Score the models using the test data\n",
    "print('SVM accuracy, no class distribution {}'.format(pipeline.score(X_test, y_test)))\n",
    "print('SVM accuracy, NearMiss undersampling {}'.format(nearmiss_pipeline.score(X_test, y_test)))\n",
    "print('SVM accuracy, SMOTE oversampling {}'.format(smote_pipeline.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Comparing performance of normal vs. SMOTE and NearMiss techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print_results(\"normal classification\", y_test, prediction)\n",
    "print()\n",
    "print_results(\"SMOTE classification\", y_test, smote_prediction)\n",
    "print()\n",
    "print_results(\"NearMiss classification\", y_test, nearmiss_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Rescaling and Normalisation (Logistic Regression and Support Vector Machines)<h3>Logistic Regression<h4> - Declaring scaling models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doing MinMax and Standard scaling and analysing results\n",
    "\n",
    "min_max = preprocessing.MinMaxScaler()\n",
    "training_minmax = min_max.fit_transform(training_data)\n",
    "\n",
    "std = preprocessing.StandardScaler()\n",
    "training_std = std.fit_transform(training_data)\n",
    "\n",
    "training_l1 = preprocessing.normalize(training_data, norm=\"l1\")\n",
    "\n",
    "training_l2 = preprocessing.normalize(training_data, norm=\"l2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Computing MSE for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE of Logistic Regression with MinMax, Scaling and Normalisation\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_data, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with nothing: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_minmax, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with MinMax: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_std, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with Standard Scaler: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_l1, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with Normalisation(L1): {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_l2, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with Normalisation(L2): {}\".format(mean_squared_error(y_test, prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # classification reports\n",
    "# print(classification_report(y_test, prediction))\n",
    "# # print(classification_report_imbalanced(y_test, nearmiss_prediction))\n",
    "# print(classification_report_imbalanced(y_test, smote_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Using scaling/normalisation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Logistic Regression with normal training data, MinMax and Standard scaling\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using Logistic Regression with training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "# print('LR training set accuracy(training data): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('Logistic Regression accuracy(training data): {}'.format(lr_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using Logistic Regression with MinMax scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "# print('Logistic Regression training set accuracy(Standard scaling): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('Logistic Regression accuracy(MinMax): {}'.format(lr_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using Logistic Regression with Standard scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "# print('Logistic Regression training set accuracy(Standard scaling): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('Logistic Regression accuracy(Standard scaling): {}'.format(lr_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using Logistic Regression with Normalisation(L1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "# print('LR training set accuracy(Normalisation, l1): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('Logistic Regression accuracy(Normalisation, l1): {}'.format(lr_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using Logistic Regression with Standard scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "# print('LR training set accuracy(Standard scaling): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('Logistic Regression accuracy(Normalisation, l2): {}'.format(lr_model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Adding sampling techniques to scaling/normalisation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling, Normalisation with class distribution techniques on Logistic Regression\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_minmax = make_pipeline_imb(SMOTE(), classifier_lr())\n",
    "smote_model_minmax = smote_pipeline_minmax.fit(X_train, y_train)\n",
    "smote_prediction_minmax = smote_model_minmax.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_std = make_pipeline_imb(SMOTE(), classifier_lr())\n",
    "smote_model_std = smote_pipeline_std.fit(X_train, y_train)\n",
    "smote_prediction_std = smote_model_std.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_l1 = make_pipeline_imb(SMOTE(), classifier_lr())\n",
    "smote_model_l1 = smote_pipeline_l1.fit(X_train, y_train)\n",
    "smote_prediction_l1 = smote_model_l1.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_l2 = make_pipeline_imb(SMOTE(), classifier_lr())\n",
    "smote_model_l2 = smote_pipeline_l2.fit(X_train, y_train)\n",
    "smote_prediction_l2 = smote_model_l2.predict(X_test)\n",
    "\n",
    "print('Logistic Regression accuracy, SMOTE, MinMax {}'.format(smote_pipeline_minmax.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, SMOTE, Standard scaling {}'.format(smote_pipeline_std.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, SMOTE, Normalisation(L1) {}'.format(smote_pipeline_l1.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, SMOTE, Normalisation(L2) {}'.format(smote_pipeline_l2.score(X_test, y_test)))\n",
    "\n",
    "print()\n",
    "#######################################################################################################################\n",
    "#######################################################################################################################\n",
    "\n",
    "# Scaling, Normalisation with class distribution techniques on Logistic Regression\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_minmax = make_pipeline_imb(NearMiss(), classifier_lr())\n",
    "nm_model_minmax = nm_pipeline_minmax.fit(X_train, y_train)\n",
    "nm_prediction_minmax = nm_model_minmax.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_std = make_pipeline_imb(NearMiss(), classifier_lr())\n",
    "nm_model_std = nm_pipeline_std.fit(X_train, y_train)\n",
    "nm_prediction_std = nm_model_std.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_l1 = make_pipeline_imb(NearMiss(), classifier_lr())\n",
    "nm_model_l1 = nm_pipeline_l1.fit(X_train, y_train)\n",
    "nm_prediction_l1 = nm_model_l1.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_l2 = make_pipeline_imb(NearMiss(), classifier_lr())\n",
    "nm_model_l2 = nm_pipeline_l2.fit(X_train, y_train)\n",
    "nm_prediction_l2 = nm_model_l2.predict(X_test)\n",
    "\n",
    "\n",
    "print('Logistic Regression accuracy, NearMiss, MinMax {}'.format(nm_pipeline_minmax.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, NearMiss, Standard scaling {}'.format(nm_pipeline_std.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, NearMiss, Normalisation(L1) {}'.format(nm_pipeline_l1.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, NearMiss, Normalisation(L2) {}'.format(nm_pipeline_l2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Support Vector Machine<h4> - Declaring scaling models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE of SVC with MinMax, Scaling and Normalisation\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_data, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with nothing: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_minmax, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with MinMax: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_std, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with Standard Scaler: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_l1, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with Normalisation(L1): {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_l2, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with Normalisation(L2): {}\".format(mean_squared_error(y_test, prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # classification reports\n",
    "# print(classification_report(y_test, prediction))\n",
    "# # print(classification_report_imbalanced(y_test, nearmiss_prediction))\n",
    "# print(classification_report_imbalanced(y_test, smote_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Using scaling/normalisation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SVMs with normal training data, MinMax and Standard scaling\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using SVMs with training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "# print('SVM training set accuracy(training data): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(training data): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "# Using SVMs with MinMax scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using SVMs on data\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "# print('SVM training set accuracy(Standard scaling): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(MinMax scaling): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "# Using SVMs with Standard scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using SVMs on data\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "# print('SVM training set accuracy(Standard scaling): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(Standard scaling): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "# Using SVMs with Normalisation(l1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using SVMs on data\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "# print('SVM training set accuracy(Normalisation, l1): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(Normalisation, l1): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "# Using SVMs with Normalisation(l2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using SVMs on data\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "# print('SVM training set accuracy(Normalisation, l1): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(Normalisation, l2): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Adding sampling techniques to scaling/normalisation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling, Normalisation with class distribution techniques on Logistic Regression\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_minmax = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model_minmax = smote_pipeline_minmax.fit(X_train, y_train)\n",
    "smote_prediction_minmax = smote_model_minmax.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_std = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model_std = smote_pipeline_std.fit(X_train, y_train)\n",
    "smote_prediction_std = smote_model_std.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_l1 = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model_l1 = smote_pipeline_l1.fit(X_train, y_train)\n",
    "smote_prediction_l1 = smote_model_l1.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_l2 = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model_l2 = smote_pipeline_l2.fit(X_train, y_train)\n",
    "smote_prediction_l2 = smote_model_l2.predict(X_test)\n",
    "\n",
    "print('Support Vector Machine accuracy, SMOTE, MinMax {}'.format(smote_pipeline_minmax.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, SMOTE, Standard scaling {}'.format(smote_pipeline_std.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, SMOTE, Normalisation(L1) {}'.format(smote_pipeline_l1.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, SMOTE, Normalisation(L2) {}'.format(smote_pipeline_l2.score(X_test, y_test)))\n",
    "\n",
    "print()\n",
    "#######################################################################################################################\n",
    "#######################################################################################################################\n",
    "\n",
    "# Scaling, Normalisation with class distribution techniques on Logistic Regression\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_minmax = make_pipeline_imb(NearMiss(), classifier_svm)\n",
    "nm_model_minmax = nm_pipeline_minmax.fit(X_train, y_train)\n",
    "nm_prediction_minmax = nm_model_minmax.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_std = make_pipeline_imb(NearMiss(), classifier_svm)\n",
    "nm_model_std = nm_pipeline_std.fit(X_train, y_train)\n",
    "nm_prediction_std = nm_model_std.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_l1 = make_pipeline_imb(NearMiss(), classifier_svm)\n",
    "nm_model_l1 = nm_pipeline_l1.fit(X_train, y_train)\n",
    "nm_prediction_l1 = nm_model_l1.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_l2 = make_pipeline_imb(NearMiss(), classifier_svm)\n",
    "nm_model_l2 = nm_pipeline_l2.fit(X_train, y_train)\n",
    "nm_prediction_l2 = nm_model_l2.predict(X_test)\n",
    "\n",
    "\n",
    "print('Support Vector Machine accuracy, NearMiss, MinMax {}'.format(nm_pipeline_minmax.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, NearMiss, Standard scaling {}'.format(nm_pipeline_std.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, NearMiss, Normalisation(L1) {}'.format(nm_pipeline_l1.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, NearMiss, Normalisation(L2) {}'.format(nm_pipeline_l2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature Selection (SelectPercentile, SelectFromModel)<h4> - SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SelectPercentile technique on Logistic Regression and scaling methods used previously\n",
    "sp_model = SelectPercentile(percentile=40)\n",
    "\n",
    "#####################################################################################################################\n",
    "# Logistic Regression, Training data, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "sp_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, using training data: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), lr_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, NearMiss, using training data: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), lr_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, SMOTE, using training data: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "########################################################\n",
    "# SVC\n",
    "print()\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, using training data: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), svm_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, NearMiss, using training data: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), svm_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, SMOTE, using training data: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "######################################################################################################################\n",
    "print()\n",
    "print()\n",
    "# Logistic Regression, Training data, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "sp_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, MinMax: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), lr_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, NearMiss, MinMax: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), lr_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, SMOTE, MinMax: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "########################################################\n",
    "# SVC\n",
    "print()\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, MinMax: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), svm_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, NearMiss, MinMax: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), svm_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, SMOTE, MinMax: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Logistic Regression, Training data, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "sp_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, Standard Scaling: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), lr_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, NearMiss, Standard Scaling: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), lr_model)\n",
    "# smote_model = smote_pipeline.fit(X_train_selected, y_train)\n",
    "# smote_prediction = smote_model.predict(X_test_selected)\n",
    "print('Logistic Regression accuracy, SelectPercentile, SMOTE, Standard Scaling: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "########################################################\n",
    "# SVC\n",
    "print()\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, Standard Scaling: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), svm_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, NearMiss, Standard Scaling: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), svm_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, SMOTE, Standard Scaling: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Logistic Regression, Training data, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "sp_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, Normalisation(L1): {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), lr_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, NearMiss, Normalisation(L1): {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), lr_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, SMOTE, Normalisation(L1): {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "########################################################\n",
    "# SVC\n",
    "print()\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, Normalisation(L1): {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), svm_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, NearMiss, Normalisation(L1): {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), svm_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, SMOTE, Normalisation(L1): {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SelectFromModel technique on Logistic Regression and scaling methods used previously\n",
    "sm_model = SelectFromModel(LinearSVC(C=0.01, penalty='l1', dual=False))\n",
    "# sm_model = SelectFromModel(LassoCV())\n",
    "\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "sm_model.fit(X_train, y_train)\n",
    "X_train_selected = sm_model.transform(X_train)\n",
    "X_test_selected = sm_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, using training data: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), lr_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, NearMiss, using training data: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), lr_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, SMOTE, using training data: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "########################################################\n",
    "# SVC\n",
    "print()\n",
    "svm_model = SVC()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "sm_model.fit(X_train, y_train)\n",
    "X_train_selected = sm_model.transform(X_train)\n",
    "X_test_selected = sm_model.transform(X_test)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), svm_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, NearMiss, using training data: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), svm_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, SMOTE, using training data: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "print('')\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "sm_model.fit(X_train, y_train)\n",
    "X_train_selected = sm_model.transform(X_train)\n",
    "X_test_selected = sm_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, MinMax: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), lr_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, NearMiss, MinMax: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), lr_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, SMOTE, MinMax: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "########################################################\n",
    "# SVC\n",
    "print()\n",
    "svm_model = SVC()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "sm_model.fit(X_train, y_train)\n",
    "X_train_selected = sm_model.transform(X_train)\n",
    "X_test_selected = sm_model.transform(X_test)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, MinMax: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), svm_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, NearMiss, MinMax: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), svm_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, SMOTE, MinMax: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "print('')\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "sm_model.fit(X_train, y_train)\n",
    "X_train_selected = sm_model.transform(X_train)\n",
    "X_test_selected = sm_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, Standard scaling: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), lr_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, NearMiss, Standard scaling: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), lr_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, SMOTE, Standard scaling: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "########################################################\n",
    "# SVC\n",
    "print()\n",
    "svm_model = SVC()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "sm_model.fit(X_train, y_train)\n",
    "X_train_selected = sm_model.transform(X_train)\n",
    "X_test_selected = sm_model.transform(X_test)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, Standard scaling: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), svm_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, NearMiss, Standard scaling: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), svm_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, SMOTE, Standard scaling: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "print('')\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "sm_model.fit(X_train, y_train)\n",
    "X_train_selected = sm_model.transform(X_train)\n",
    "X_test_selected = sm_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, (Normalisation, l1): {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), lr_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, NearMiss, (Normalisation, l1): {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), lr_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, SMOTE, (Normalisation, l1): {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "########################################################\n",
    "# SVC\n",
    "print()\n",
    "svm_model = SVC()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "sm_model.fit(X_train, y_train)\n",
    "X_train_selected = sm_model.transform(X_train)\n",
    "X_test_selected = sm_model.transform(X_test)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, (Normalisation, l1): {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), svm_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, NearMiss, (Normalisation, l1): {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), svm_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, SMOTE, (Normalisation, l1): {}'.format(smote_pipeline.score(X_test_selected, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> PCA Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines\n",
    "estimators = [('reduce_dim', PCA(n_components=10)), ('clf', SVC())]\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train) \n",
    "print('RBF Kernel, MinMax scaling, PCA(10 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('RBF Kernel, Standard scaling, PCA(10 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('RBF Kernel, Normalisation(L1), PCA(10 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "estimators = [('reduce_dim', PCA(n_components=10)), ('clf', SVC(kernel='linear'))]\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Linear Kernel, MinMax scaling, PCA(10 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Linear Kernel, Standard scaling, PCA(10 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Linear Kernel, Normalisation(L1), PCA(10 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "estimators = [('reduce_dim', PCA(n_components=5)), ('clf', SVC())]\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('RBF Kernel, MinMax scaling, PCA(5 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('RBF Kernel, Standard scaling, PCA(5 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('RBF Kernel, Normalisation(L1), PCA(5 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "estimators = [('reduce_dim', PCA(n_components=5)), ('clf', SVC(kernel='linear'))]\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Linear Kernel, MinMax scaling, PCA(5 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Linear Kernel, Standard scaling, PCA(5 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Linear Kernel, Normalisation(L1), PCA(5 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "\n",
    "pca = PCA(.95)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pca.fit(X_train)\n",
    "print('Retaining 95% variance with {} components'.format(pca.n_components_))\n",
    "########################################################\n",
    "########################################################\n",
    "print('')\n",
    "pca = PCA(.85)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pca.fit(X_train)\n",
    "print('Retaining 85% variance with {} components'.format(pca.n_components_))\n",
    "print('')\n",
    "######################################################################################################################\n",
    "estimators = [('reduce_dim', PCA(n_components=3)), ('clf', SVC())]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('RBF Kernel, MinMax scaling, PCA(3 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('RBF Kernel, Standard scaling scaling, PCA(3 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('RBF Kernel, Normalisation(L1), PCA(3 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "#####################################################################################################################\n",
    "estimators = [('reduce_dim', PCA(n_components=3)), ('clf', SVC(kernel='linear'))]\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Linear Kernel, MinMax scaling, PCA(3 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Linear Kernel, Standard scaling, PCA(3 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Linear Kernel, Normalisation(L1), PCA(3 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-a7410634f599>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-a7410634f599>\"\u001b[1;36m, line \u001b[1;32m23\u001b[0m\n\u001b[1;33m    svm = SVC(kernel='linear')\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 10-Fold Cross validation\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "scores = cross_val_score(logreg, training_std, target_data, cv=10)\n",
    "print('Logistic regression, 10-fold cross-validation: %0.3f (+/- (%.3f))' % (scores.mean(), scores.std()*2))\n",
    "\n",
    "\n",
    "# svm = SVC(kernel='linear')\n",
    "# scores = cross_val_score(svm, training_std, target_data, cv=10)\n",
    "# print('Linear kernel SVM, 10-fold cross-validation: %0.3f (+/- (%.3f))' % (scores.mean(), scores.std()*2))\n",
    "\n",
    "# svm = SVC()\n",
    "# scores = cross_val_score(svm, training_std, target_data, cv=10)\n",
    "# print('RBF Kernel, 10-fold cross-validation: %0.3f (+/- (%.3f))' % (scores.mean(), scores.std()*2))\n",
    "\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "logreg = LogisticRegression()\n",
    "scores = cross_val_score(logreg, training_std, target_data, cv=10, scoring='recall')\n",
    "print('Logistic regression recall, 10-fold cross-validation: %0.3f)' % (scores.mean())\n",
    "\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "scores = cross_val_score(svm, training_std, target_data, cv=10, scoring='recall')\n",
    "print('Linear kernel SVM recall, 10-fold cross-validation: %0.3f)' % (scores.mean())\n",
    "\n",
    "svm = SVC()\n",
    "scores = cross_val_score(svm, training_std, target_data, cv=10, scoring='recall')\n",
    "print('RBF Kernel recall, 10-fold cross-validation: %0.3f)' % (scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Printing mean of several metrics after 5-fold cross validation\n",
    "# print(\"Mean of scores 5-fold:\")\n",
    "# print(\"Accuracy: {}\".format(np.mean(accuracy)))\n",
    "# print(\"Precision: {}\".format(np.mean(precision)))\n",
    "# print(\"Recall: {}\".format(np.mean(recall)))\n",
    "# print(\"F1: {}\".format(np.mean(f1)))\n",
    "# print(\"Auc: {}\".format(np.mean(auc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper-parameter tuning\n",
    "#####################################################################################################################\n",
    "\n",
    "# GridSearch\n",
    "# tuned_parameters = [\n",
    "#     {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [0.01, 0.1, 1]},\n",
    "#     {'kernel': ['linear'], 'C': [0.01, 0.1, 1]}\n",
    "# ]\n",
    "\n",
    "# clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=10, scoring='f1_macro')\n",
    "\n",
    "# print('Training data')\n",
    "# X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "# clf.fit(X_train, y_train)\n",
    "# clf.best_params_\n",
    "# y_true, y_pred = y_test, clf.predict(X_test)\n",
    "\n",
    "# print('Standard scaling')\n",
    "# X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "# clf.fit(X_train, y_train)\n",
    "# clf.best_params_\n",
    "# y_true, y_pred = y_test, clf.predict(X_test)\n",
    "\n",
    "# print('Normalisation L1')\n",
    "# X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "# clf.fit(X_train, y_train)\n",
    "# clf.best_params_\n",
    "# y_true, y_pred = y_test, clf.predict(X_test)\n",
    "\n",
    "\n",
    "# print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
