{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scoring metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# model selection stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import scipy\n",
    "# scaling and normalisation metrics\n",
    "from sklearn import preprocessing\n",
    "# classifier models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.svm import SVC\n",
    "# graphical stuff\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# feature selection\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "# dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# helper function to print the data\n",
    "def print_results(headline, true_value, pred):\n",
    "    print(headline)\n",
    "    print(\"accuracy: {}\".format(accuracy_score(true_value, pred)))\n",
    "    print(\"precision: {}\".format(precision_score(true_value, pred)))\n",
    "    print(\"recall: {}\".format(recall_score(true_value, pred)))\n",
    "    print(\"f1: {}\".format(f1_score(true_value, pred)))\n",
    "\n",
    "# helper function to build the model\n",
    "def build_lr_model_for_data(data, target):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42, test_size=0.2)\n",
    "    pipeline = make_pipeline(LogisticRegression())\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    return (X_test, y_test, model)\n",
    "\n",
    "# helper function to build the model\n",
    "def build_svc_model_for_data(data, target):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42, test_size=0.2)\n",
    "    pipeline = make_pipeline(SVC())\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    return (X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: (30000, 25)\n",
      "after: (30000, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0      X1 X2 X3 X4  X5  X6 X7  X8  X9 ...    X15    X16    X17  \\\n",
       "1          1   20000  2  2  1  24   2  2  -1  -1 ...      0      0      0   \n",
       "2          2  120000  2  2  2  26  -1  2   0   0 ...   3272   3455   3261   \n",
       "3          3   90000  2  2  2  34   0  0   0   0 ...  14331  14948  15549   \n",
       "4          4   50000  2  2  1  37   0  0   0   0 ...  28314  28959  29547   \n",
       "5          5   50000  1  2  1  57  -1  0  -1   0 ...  20940  19146  19131   \n",
       "\n",
       "    X18    X19    X20   X21   X22   X23  Y  \n",
       "1     0    689      0     0     0     0  1  \n",
       "2     0   1000   1000  1000     0  2000  1  \n",
       "3  1518   1500   1000  1000  1000  5000  0  \n",
       "4  2000   2019   1200  1100  1069  1000  0  \n",
       "5  2000  36681  10000  9000   689   679  0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data and remove the feature titles\n",
    "df = pd.read_csv('cc_clients.csv')\n",
    "data = df.drop(df.index[[0]])\n",
    "print('before: {}'.format(data.shape))\n",
    "data.drop_duplicates()\n",
    "print('after: {}'.format(data.shape))\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate the data into features and target\n",
    "training_data = np.asarray(data.loc[:, 'X1':'X23']).astype(np.float)\n",
    "target_data = np.asarray(data['Y']).astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Statistics to see how the dataset is transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal data distribution: Counter({0.0: 23364, 1.0: 6636})\n",
      "NearMiss(Undersampling) data distribution: Counter({0.0: 6636, 1.0: 6636})\n",
      "SMOTE(Oversampling) data distribution: Counter({1.0: 23364, 0.0: 23364})\n"
     ]
    }
   ],
   "source": [
    "# Results from over/undersampling vs. normal distribution\n",
    "print(\"Normal data distribution: {}\".format(Counter(target_data)))\n",
    "\n",
    "X_nearmiss, y_nearmiss = NearMiss().fit_sample(training_data, target_data)\n",
    "print(\"NearMiss(Undersampling) data distribution: {}\".format(Counter(y_nearmiss)))\n",
    "\n",
    "X_smote, y_smote = SMOTE().fit_sample(training_data, target_data)\n",
    "print(\"SMOTE(Oversampling) data distribution: {}\".format(Counter(y_smote)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare classification models(LR, SVM)\n",
    "classifier_lr = LogisticRegression\n",
    "classifier_svm = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NearMiss undersampling vs SMOTE oversampling(Logistic Regression and Support Vector Machines)<h4> - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy, no class distribution 0.7811666666666667\n",
      "Logistic Regression accuracy, NearMiss undersampling 0.336\n",
      "Logistic Regression accuracy, SMOTE oversampling 0.6158333333333333\n"
     ]
    }
   ],
   "source": [
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normal model\n",
    "pipeline = make_pipeline(classifier_lr(random_state=42))\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nearmiss_pipeline = make_pipeline_imb(NearMiss(random_state=42), classifier_lr(random_state=42))\n",
    "nearmiss_model = nearmiss_pipeline.fit(X_train, y_train)\n",
    "nearmiss_prediction = nearmiss_model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(random_state=42), classifier_lr(random_state=42))\n",
    "smote_model = smote_pipeline.fit(X_train, y_train)\n",
    "smote_prediction = smote_model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing) \n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Score the models using the test data\n",
    "print('Logistic Regression accuracy, no class distribution {}'.format(pipeline.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, NearMiss undersampling {}'.format(nearmiss_pipeline.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, SMOTE oversampling {}'.format(smote_pipeline.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal classification\n",
      "accuracy: 0.7811666666666667\n",
      "precision: 0.5\n",
      "recall: 0.0007616146230007616\n",
      "f1: 0.0015209125475285172\n",
      "\n",
      "SMOTE classification\n",
      "accuracy: 0.6158333333333333\n",
      "precision: 0.3218390804597701\n",
      "recall: 0.6824067022086824\n",
      "f1: 0.4373932145472297\n",
      "\n",
      "NearMiss classification\n",
      "accuracy: 0.336\n",
      "precision: 0.18134096874254355\n",
      "recall: 0.5788271134805788\n",
      "f1: 0.2761627906976744\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print_results(\"normal classification\", y_test, prediction)\n",
    "print()\n",
    "print_results(\"SMOTE classification\", y_test, smote_prediction)\n",
    "print()\n",
    "print_results(\"NearMiss classification\", y_test, nearmiss_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy, no class distribution 0.7776666666666666\n",
      "SVM accuracy, NearMiss undersampling 0.7776666666666666\n",
      "SVM accuracy, SMOTE oversampling 0.7776666666666666\n"
     ]
    }
   ],
   "source": [
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normal model\n",
    "pipeline = make_pipeline(classifier_svm)\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nearmiss_pipeline = make_pipeline_imb(NearMiss(random_state=42), classifier_svm)\n",
    "nearmiss_model = nearmiss_pipeline.fit(X_train, y_train)\n",
    "nearmiss_prediction = nearmiss_model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model = smote_pipeline.fit(X_train, y_train)\n",
    "smote_prediction = smote_model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Score the models using the test data\n",
    "print('SVM accuracy, no class distribution {}'.format(pipeline.score(X_test, y_test)))\n",
    "print('SVM accuracy, NearMiss undersampling {}'.format(nearmiss_pipeline.score(X_test, y_test)))\n",
    "print('SVM accuracy, SMOTE oversampling {}'.format(smote_pipeline.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Comparing performance of normal vs. SMOTE and NearMiss techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal classification\n",
      "accuracy: 0.7806666666666666\n",
      "precision: 0.45161290322580644\n",
      "recall: 0.010662604722010662\n",
      "f1: 0.020833333333333336\n",
      "\n",
      "SMOTE classification\n",
      "accuracy: 0.7776666666666666\n",
      "precision: 0.37349397590361444\n",
      "recall: 0.02361005331302361\n",
      "f1: 0.044412607449856735\n",
      "\n",
      "NearMiss classification\n",
      "accuracy: 0.224\n",
      "precision: 0.21548936170212765\n",
      "recall: 0.9642041127189642\n",
      "f1: 0.3522537562604341\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print_results(\"normal classification\", y_test, prediction)\n",
    "print()\n",
    "print_results(\"SMOTE classification\", y_test, smote_prediction)\n",
    "print()\n",
    "print_results(\"NearMiss classification\", y_test, nearmiss_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Rescaling and Normalisation (Logistic Regression and Support Vector Machines)<h3>Logistic Regression<h4> - Declaring scaling models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doing MinMax and Standard scaling and analysing results\n",
    "\n",
    "min_max = preprocessing.MinMaxScaler()\n",
    "training_minmax = min_max.fit_transform(training_data)\n",
    "\n",
    "std = preprocessing.StandardScaler()\n",
    "training_std = std.fit_transform(training_data)\n",
    "\n",
    "training_l1 = preprocessing.normalize(training_data, norm=\"l1\")\n",
    "\n",
    "training_l2 = preprocessing.normalize(training_data, norm=\"l2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Computing MSE for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Logistic Regression with nothing: 0.21883333333333332\n",
      "MSE of Logistic Regression with MinMax: 0.19\n",
      "MSE of Logistic Regression with Standard Scaler: 0.19016666666666668\n",
      "MSE of Logistic Regression with Normalisation(L1): 0.21883333333333332\n",
      "MSE of Logistic Regression with Normalisation(L2): 0.21916666666666668\n"
     ]
    }
   ],
   "source": [
    "# MSE of Logistic Regression with MinMax, Scaling and Normalisation\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_data, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with nothing: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_minmax, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with MinMax: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_std, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with Standard Scaler: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_l1, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with Normalisation(L1): {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_l2, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with Normalisation(L2): {}\".format(mean_squared_error(y_test, prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # classification reports\n",
    "# print(classification_report(y_test, prediction))\n",
    "# # print(classification_report_imbalanced(y_test, nearmiss_prediction))\n",
    "# print(classification_report_imbalanced(y_test, smote_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Using scaling/normalisation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy(training data): 0.7811666666666667\n",
      "Logistic Regression accuracy(MinMax): 0.81\n",
      "Logistic Regression accuracy(Standard scaling): 0.8098333333333333\n",
      "Logistic Regression accuracy(Normalisation, l1): 0.7811666666666667\n",
      "Logistic Regression accuracy(Normalisation, l2): 0.7808333333333334\n"
     ]
    }
   ],
   "source": [
    "# Using Logistic Regression with normal training data, MinMax and Standard scaling\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using Logistic Regression with training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "# print('LR training set accuracy(training data): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('Logistic Regression accuracy(training data): {}'.format(lr_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using Logistic Regression with MinMax scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "# print('Logistic Regression training set accuracy(Standard scaling): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('Logistic Regression accuracy(MinMax): {}'.format(lr_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using Logistic Regression with Standard scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "# print('Logistic Regression training set accuracy(Standard scaling): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('Logistic Regression accuracy(Standard scaling): {}'.format(lr_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using Logistic Regression with Normalisation(L1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "# print('LR training set accuracy(Normalisation, l1): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('Logistic Regression accuracy(Normalisation, l1): {}'.format(lr_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using Logistic Regression with Standard scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "# print('LR training set accuracy(Standard scaling): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('Logistic Regression accuracy(Normalisation, l2): {}'.format(lr_model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Adding sampling techniques to scaling/normalisation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy, SMOTE, MinMax 0.7811666666666667\n",
      "Logistic Regression accuracy, SMOTE, Standard scaling 0.7811666666666667\n",
      "Logistic Regression accuracy, SMOTE, Normalisation(L1) 0.43216666666666664\n",
      "Logistic Regression accuracy, SMOTE, Normalisation(L2) 0.5931666666666666\n",
      "\n",
      "Logistic Regression accuracy, NearMiss, MinMax 0.3978333333333333\n",
      "Logistic Regression accuracy, NearMiss, Standard scaling 0.21883333333333332\n",
      "Logistic Regression accuracy, NearMiss, Normalisation(L1) 0.7408333333333333\n",
      "Logistic Regression accuracy, NearMiss, Normalisation(L2) 0.3695\n"
     ]
    }
   ],
   "source": [
    "# Scaling, Normalisation with class distribution techniques on Logistic Regression\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_minmax = make_pipeline_imb(SMOTE(), classifier_lr())\n",
    "smote_model_minmax = smote_pipeline_minmax.fit(X_train, y_train)\n",
    "smote_prediction_minmax = smote_model_minmax.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_std = make_pipeline_imb(SMOTE(), classifier_lr())\n",
    "smote_model_std = smote_pipeline_std.fit(X_train, y_train)\n",
    "smote_prediction_std = smote_model_std.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_l1 = make_pipeline_imb(SMOTE(), classifier_lr())\n",
    "smote_model_l1 = smote_pipeline_l1.fit(X_train, y_train)\n",
    "smote_prediction_l1 = smote_model_l1.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_l2 = make_pipeline_imb(SMOTE(), classifier_lr())\n",
    "smote_model_l2 = smote_pipeline_l2.fit(X_train, y_train)\n",
    "smote_prediction_l2 = smote_model_l2.predict(X_test)\n",
    "\n",
    "print('Logistic Regression accuracy, SMOTE, MinMax {}'.format(smote_pipeline_minmax.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, SMOTE, Standard scaling {}'.format(smote_pipeline_std.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, SMOTE, Normalisation(L1) {}'.format(smote_pipeline_l1.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, SMOTE, Normalisation(L2) {}'.format(smote_pipeline_l2.score(X_test, y_test)))\n",
    "\n",
    "print()\n",
    "#######################################################################################################################\n",
    "#######################################################################################################################\n",
    "\n",
    "# Scaling, Normalisation with class distribution techniques on Logistic Regression\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_minmax = make_pipeline_imb(NearMiss(), classifier_lr())\n",
    "nm_model_minmax = nm_pipeline_minmax.fit(X_train, y_train)\n",
    "nm_prediction_minmax = nm_model_minmax.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_std = make_pipeline_imb(NearMiss(), classifier_lr())\n",
    "nm_model_std = nm_pipeline_std.fit(X_train, y_train)\n",
    "nm_prediction_std = nm_model_std.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_l1 = make_pipeline_imb(NearMiss(), classifier_lr())\n",
    "nm_model_l1 = nm_pipeline_l1.fit(X_train, y_train)\n",
    "nm_prediction_l1 = nm_model_l1.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_l2 = make_pipeline_imb(NearMiss(), classifier_lr())\n",
    "nm_model_l2 = nm_pipeline_l2.fit(X_train, y_train)\n",
    "nm_prediction_l2 = nm_model_l2.predict(X_test)\n",
    "\n",
    "\n",
    "print('Logistic Regression accuracy, NearMiss, MinMax {}'.format(nm_pipeline_minmax.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, NearMiss, Standard scaling {}'.format(nm_pipeline_std.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, NearMiss, Normalisation(L1) {}'.format(nm_pipeline_l1.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, NearMiss, Normalisation(L2) {}'.format(nm_pipeline_l2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Support Vector Machine<h4> - Declaring scaling models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Support Vector Machine with nothing: 0.21933333333333332\n",
      "MSE of Support Vector Machine with MinMax: 0.212\n",
      "MSE of Support Vector Machine with Standard Scaler: 0.18066666666666667\n",
      "MSE of Support Vector Machine with Normalisation(L1): 0.21883333333333332\n",
      "MSE of Support Vector Machine with Normalisation(L2): 0.21883333333333332\n"
     ]
    }
   ],
   "source": [
    "# MSE of SVC with MinMax, Scaling and Normalisation\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_data, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with nothing: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_minmax, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with MinMax: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_std, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with Standard Scaler: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_l1, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with Normalisation(L1): {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_l2, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with Normalisation(L2): {}\".format(mean_squared_error(y_test, prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # classification reports\n",
    "# print(classification_report(y_test, prediction))\n",
    "# # print(classification_report_imbalanced(y_test, nearmiss_prediction))\n",
    "# print(classification_report_imbalanced(y_test, smote_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Using scaling/normalisation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM test set accuracy(training data): 0.7791666666666667\n",
      "SVM test set accuracy(MinMax scaling): 0.789\n",
      "SVM test set accuracy(Standard scaling): 0.8158333333333333\n",
      "SVM test set accuracy(Normalisation, l1): 0.7788333333333334\n",
      "SVM test set accuracy(Normalisation, l2): 0.7788333333333334\n"
     ]
    }
   ],
   "source": [
    "# Using SVMs with normal training data, MinMax and Standard scaling\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using SVMs with training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "# print('SVM training set accuracy(training data): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(training data): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "# Using SVMs with MinMax scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using SVMs on data\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "# print('SVM training set accuracy(Standard scaling): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(MinMax scaling): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "# Using SVMs with Standard scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using SVMs on data\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "# print('SVM training set accuracy(Standard scaling): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(Standard scaling): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "# Using SVMs with Normalisation(l1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using SVMs on data\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "# print('SVM training set accuracy(Normalisation, l1): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(Normalisation, l1): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "# Using SVMs with Normalisation(l2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using SVMs on data\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "# print('SVM training set accuracy(Normalisation, l1): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(Normalisation, l2): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Adding sampling techniques to scaling/normalisation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine accuracy, SMOTE, MinMax 0.5906666666666667\n",
      "Support Vector Machine accuracy, SMOTE, Standard scaling 0.5906666666666667\n",
      "Support Vector Machine accuracy, SMOTE, Normalisation(L1) 0.5906666666666667\n",
      "Support Vector Machine accuracy, SMOTE, Normalisation(L2) 0.5906666666666667\n",
      "\n",
      "Support Vector Machine accuracy, NearMiss, MinMax 0.44\n",
      "Support Vector Machine accuracy, NearMiss, Standard scaling 0.44\n",
      "Support Vector Machine accuracy, NearMiss, Normalisation(L1) 0.44\n",
      "Support Vector Machine accuracy, NearMiss, Normalisation(L2) 0.44\n"
     ]
    }
   ],
   "source": [
    "# Scaling, Normalisation with class distribution techniques on Logistic Regression\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_minmax = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model_minmax = smote_pipeline_minmax.fit(X_train, y_train)\n",
    "smote_prediction_minmax = smote_model_minmax.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_std = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model_std = smote_pipeline_std.fit(X_train, y_train)\n",
    "smote_prediction_std = smote_model_std.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_l1 = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model_l1 = smote_pipeline_l1.fit(X_train, y_train)\n",
    "smote_prediction_l1 = smote_model_l1.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_l2 = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model_l2 = smote_pipeline_l2.fit(X_train, y_train)\n",
    "smote_prediction_l2 = smote_model_l2.predict(X_test)\n",
    "\n",
    "print('Support Vector Machine accuracy, SMOTE, MinMax {}'.format(smote_pipeline_minmax.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, SMOTE, Standard scaling {}'.format(smote_pipeline_std.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, SMOTE, Normalisation(L1) {}'.format(smote_pipeline_l1.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, SMOTE, Normalisation(L2) {}'.format(smote_pipeline_l2.score(X_test, y_test)))\n",
    "\n",
    "print()\n",
    "#######################################################################################################################\n",
    "#######################################################################################################################\n",
    "\n",
    "# Scaling, Normalisation with class distribution techniques on Logistic Regression\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_minmax = make_pipeline_imb(NearMiss(), classifier_svm)\n",
    "nm_model_minmax = nm_pipeline_minmax.fit(X_train, y_train)\n",
    "nm_prediction_minmax = nm_model_minmax.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_std = make_pipeline_imb(NearMiss(), classifier_svm)\n",
    "nm_model_std = nm_pipeline_std.fit(X_train, y_train)\n",
    "nm_prediction_std = nm_model_std.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_l1 = make_pipeline_imb(NearMiss(), classifier_svm)\n",
    "nm_model_l1 = nm_pipeline_l1.fit(X_train, y_train)\n",
    "nm_prediction_l1 = nm_model_l1.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_l2 = make_pipeline_imb(NearMiss(), classifier_svm)\n",
    "nm_model_l2 = nm_pipeline_l2.fit(X_train, y_train)\n",
    "nm_prediction_l2 = nm_model_l2.predict(X_test)\n",
    "\n",
    "\n",
    "print('Support Vector Machine accuracy, NearMiss, MinMax {}'.format(nm_pipeline_minmax.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, NearMiss, Standard scaling {}'.format(nm_pipeline_std.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, NearMiss, Normalisation(L1) {}'.format(nm_pipeline_l1.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, NearMiss, Normalisation(L2) {}'.format(nm_pipeline_l2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature Selection (SelectPercentile, SelectFromModel)<h4> - SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 0.7811666666666667\n",
      "Logistic Regression accuracy, SelectPercentile: 0.7811666666666667\n",
      "Logistic Regression accuracy, SelectPercentile, NearMiss: 0.336\n",
      "Logistic Regression accuracy, SelectPercentile, SMOTE: 0.6026666666666667\n",
      "Support Vector Machine accuracy, SelectPercentile: 0.7801666666666667\n",
      "Support Vector Machine accuracy, SelectPercentile, NearMiss: 0.224\n",
      "Support Vector Machine accuracy, SelectPercentile, SMOTE: 0.224\n",
      "\n",
      "Logistic Regression accuracy, MinMax: 0.81\n",
      "Logistic Regression accuracy, SelectPercentile, MinMax: 0.8086666666666666\n",
      "Support Vector Machine accuracy, SelectPercentile, MinMax: 0.805\n",
      "\n",
      "Logistic Regression accuracy, Standard scaling: 0.8098333333333333\n",
      "Logistic Regression accuracy, SelectPercentile, Standard scaling: 0.8096666666666666\n",
      "Support Vector Machine accuracy, SelectPercentile, Standard scaling: 0.8208333333333333\n",
      "\n",
      "Logistic Regression accuracy, (Normalisation, l1): 0.7811666666666667\n",
      "Logistic Regression accuracy, SelectPercentile, (Normalisation, l1): 0.7811666666666667\n",
      "Support Vector Machine accuracy, SelectPercentile, (Normalisation, l1): 0.7811666666666667\n",
      "\n",
      "Logistic Regression accuracy, (Normalisation, l2): 0.7808333333333334\n",
      "Logistic Regression accuracy, SelectPercentile, (Normalisation, l2): 0.7811666666666667\n",
      "Support Vector Machine accuracy, SelectPercentile, (Normalisation, l2): 0.7811666666666667\n"
     ]
    }
   ],
   "source": [
    "# Using SelectPercentile technique on Logistic Regression and scaling methods used previously\n",
    "sp_model = SelectPercentile(percentile=40)\n",
    "\n",
    "#####################################################################################################################\n",
    "# Logistic Regression, Training data, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "print('Logistic Regression accuracy: {}'.format(lr_model.score(X_test, y_test)))\n",
    "\n",
    "sp_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), lr_model)\n",
    "nm_model = nm_pipeline.fit(X_train, y_train)\n",
    "nm_prediction = nm_model.predict(X_test)\n",
    "print('Logistic Regression accuracy, SelectPercentile, NearMiss: {}'.format(nm_pipeline.score(X_test, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), lr_model)\n",
    "smote_model = smote_pipeline.fit(X_train, y_train)\n",
    "smote_prediction = smote_model.predict(X_test)\n",
    "print('Logistic Regression accuracy, SelectPercentile, SMOTE: {}'.format(smote_pipeline.score(X_test, y_test)))\n",
    "\n",
    "########################################################\n",
    "# SVC\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), svm_model)\n",
    "nm_model = nm_pipeline.fit(X_train, y_train)\n",
    "nm_prediction = nm_model.predict(X_test)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, NearMiss: {}'.format(nm_pipeline.score(X_test, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), svm_model)\n",
    "smote_model = nm_pipeline_l2.fit(X_train, y_train)\n",
    "smote_prediction = nm_model_l2.predict(X_test)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, SMOTE: {}'.format(smote_pipeline.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "print()\n",
    "# Logistic Regression, MinMax scaling, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "print('Logistic Regression accuracy, MinMax: {}'.format(lr_model.score(X_test, y_test)))\n",
    "\n",
    "sp_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, MinMax: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, MinMax: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "print()\n",
    "# Logistic Regression, Standard scaling, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "print('Logistic Regression accuracy, Standard scaling: {}'.format(lr_model.score(X_test, y_test)))\n",
    "\n",
    "sp_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, Standard scaling: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, Standard scaling: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "print()\n",
    "# Logistic Regression, L1 normalisation, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, random_state=42, test_size=0.2)\n",
    "  \n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "print('Logistic Regression accuracy, (Normalisation, l1): {}'.format(lr_model.score(X_test, y_test)))\n",
    "\n",
    "sp_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, (Normalisation, l1): {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, (Normalisation, l1): {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "print()\n",
    "# Logistic Regression , L2 normalisation, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, random_state=42, test_size=0.2)\n",
    "  \n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "print('Logistic Regression accuracy, (Normalisation, l2): {}'.format(lr_model.score(X_test, y_test)))\n",
    "\n",
    "sp_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, (Normalisation, l2): {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, (Normalisation, l2): {}'.format(svm_model.score(X_test_selected, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 0.7811666666666667\n",
      "Logistic Regression accuracy, SelectFromModel: 0.8078333333333333\n",
      "Support Vector Machine accuracy, SelectFromModel: 0.8195\n",
      "\n",
      "Logistic Regression accuracy: 0.81\n",
      "Logistic Regression accuracy, SelectFromModel, MinMax: 0.8091666666666667\n",
      "Support Vector Machine accuracy, SelectFromModel, MinMax: 0.8046666666666666\n",
      "\n",
      "Logistic Regression accuracy, Standard scaling: 0.8098333333333333\n",
      "Logistic Regression accuracy, SelectFromModel, Standard scaling: 0.8085\n",
      "Support Vector Machine accuracy, SelectFromModel, Standard scaling: 0.8196666666666667\n",
      "\n",
      "Logistic Regression accuracy, (Normalisation, l1): 0.7811666666666667\n",
      "Logistic Regression accuracy, SelectFromModel, (Normalisation, l1): 0.7811666666666667\n",
      "Support Vector Machine accuracy, SelectFromModel, (Normalisation, l1): 0.7811666666666667\n",
      "\n",
      "Logistic Regression accuracy, (Normalisation, l2): 0.7808333333333334\n",
      "Logistic Regression accuracy, SelectFromModel, (Normalisation, l2): 0.7808333333333334\n",
      "Support Vector Machine accuracy, SelectFromModel, (Normalisation, l2): 0.7811666666666667\n"
     ]
    }
   ],
   "source": [
    "# Using SelectFromModel technique on Logistic Regression and scaling methods used previously\n",
    "model = SelectFromModel(LogisticRegression(C=0.01, dual=False))\n",
    "\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "print('Logistic Regression accuracy: {}'.format(lr_model.score(X_test, y_test)))\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "X_train_selected = model.transform(X_train)\n",
    "X_test_selected = model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C = 1.0)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "print('Logistic Regression accuracy: {}'.format(lr_model.score(X_test, y_test)))\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "X_train_selected = model.transform(X_train)\n",
    "X_test_selected = model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, MinMax: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C = 1.0)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, MinMax: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "print('Logistic Regression accuracy, Standard scaling: {}'.format(lr_model.score(X_test, y_test)))\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "X_train_selected = model.transform(X_train)\n",
    "X_test_selected = model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, Standard scaling: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C = 1.0)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, Standard scaling: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "print('Logistic Regression accuracy, (Normalisation, l1): {}'.format(lr_model.score(X_test, y_test)))\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "X_train_selected = model.transform(X_train)\n",
    "X_test_selected = model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, (Normalisation, l1): {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C = 1.0)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, (Normalisation, l1): {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "print('Logistic Regression accuracy, (Normalisation, l2): {}'.format(lr_model.score(X_test, y_test)))\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "X_train_selected = model.transform(X_train)\n",
    "X_test_selected = model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, (Normalisation, l2): {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C = 1.0)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, (Normalisation, l2): {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> PCA Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA with 10 components: 0.7803333333333333\n",
      "PCA with 10 components: 0.8031666666666667\n",
      "PCA with 10 components: 0.8045\n",
      "PCA with 10 components: 0.7788333333333334\n",
      "\n",
      "PCA with 10 components: 0.779\n",
      "PCA with 10 components: 0.8193333333333334\n",
      "PCA with 10 components: 0.8065\n",
      "PCA with 10 components: 0.779\n",
      "\n",
      "PCA with 5 components: 0.7803333333333333\n",
      "PCA with 5 components: 0.7998333333333333\n",
      "PCA with 5 components: 0.8041666666666667\n",
      "PCA with 5 components: 0.7788333333333334\n"
     ]
    }
   ],
   "source": [
    "# Pipelines\n",
    "estimators = [('reduce_dim', PCA(n_components=10)), ('clf', SVC())]\n",
    "\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 10 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 10 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 10 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 10 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "estimators = [('reduce_dim', PCA(n_components=10, whiten=True)), ('clf', SVC())]\n",
    "\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 10 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 10 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 10 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 10 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "estimators = [('reduce_dim', PCA(n_components=5)), ('clf', SVC())]\n",
    "\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 5 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 5 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 5 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 5 components: {}'.format(pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Installation\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameter tuning\n",
    "#####################################################################################################################\n",
    "\n",
    "# GridSearch\n",
    "tuned_parameters = [\n",
    "    {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [0.01, 0.1, 1]},\n",
    "    {'kernel': ['linear'], 'C': [0.01, 0.1, 1]}\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=10, scoring='f1_macro')\n",
    "\n",
    "print('Training data')\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.best_params_\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "\n",
    "print('Standard scaling')\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.best_params_\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "\n",
    "print('Normalisation L1')\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.best_params_\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model evaluation(chateau)\n",
    "\n",
    "# GridSearchCV and cross_val_score take different scoring parameters\n",
    "# classification report\n",
    "# confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> To do next..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feat evaluation\n",
    "\n",
    "# add SMOTE to experiments\n",
    "# cross validation\n",
    "# visualisation(scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "logreg = LogisticRegression()\n",
    "print('Logistic regression cross-validation accuracy: %0.4f' % cross_val_score(logreg, training_data, target_data, cv=10, scoring='accuracy').mean())\n",
    "\n",
    "svm = svm.SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(clf, training_data, target_data, cv=10, scoring='accuracy')\n",
    "print('SVM cross-validation accuracy: %0.2f' % (cross_val_score(svm, training_data, target_data, cv=10, scoring='accuracy').mean())\n",
    "#####################################################################################################################\n",
    "\n",
    "# 10-Fold Cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "auc = []\n",
    "for train, test in kf.split(X_train, y_train):\n",
    "    pipeline = make_pipeline_imb(SMOTE(), classifier_lr(random_state=42))\n",
    "    model = pipeline.fit(X_train[train], y_train[train])\n",
    "    prediction = model.predict(X_train[test])\n",
    "\n",
    "    accuracy.append(pipeline.score(X_train[test], y_train[test]))\n",
    "    precision.append(precision_score(y_train[test], prediction))\n",
    "    recall.append(recall_score(y_train[test], prediction))\n",
    "    f1.append(f1_score(y_train[test], prediction))\n",
    "    auc.append(roc_auc_score(y_train[test], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Printing mean of several metrics after 5-fold cross validation\n",
    "print(\"Mean of scores 5-fold:\")\n",
    "print(\"Accuracy: {}\".format(np.mean(accuracy)))\n",
    "print(\"Precision: {}\".format(np.mean(precision)))\n",
    "print(\"Recall: {}\".format(np.mean(recall)))\n",
    "print(\"F1: {}\".format(np.mean(f1)))\n",
    "print(\"Auc: {}\".format(np.mean(auc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
