{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scoring metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# model selection stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import scipy\n",
    "# scaling and normalisation metrics\n",
    "from sklearn import preprocessing\n",
    "# classifier models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "# graphical stuff\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# feature selection\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "# dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# helper function to print the data\n",
    "def print_results(headline, true_value, pred):\n",
    "    print(headline)\n",
    "    print(\"accuracy: {}\".format(accuracy_score(true_value, pred)))\n",
    "    print(\"precision: {}\".format(precision_score(true_value, pred)))\n",
    "    print(\"recall: {}\".format(recall_score(true_value, pred)))\n",
    "    print(\"f1: {}\".format(f1_score(true_value, pred)))\n",
    "\n",
    "# helper function to build the model\n",
    "def build_lr_model_for_data(data, target):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42, test_size=0.2)\n",
    "    pipeline = make_pipeline(LogisticRegression())\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    return (X_test, y_test, model)\n",
    "\n",
    "# helper function to build the model\n",
    "def build_svc_model_for_data(data, target):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42, test_size=0.2)\n",
    "    pipeline = make_pipeline(SVC())\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    return (X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: (30000, 25)\n",
      "after: (30000, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0      X1 X2 X3 X4  X5  X6 X7  X8  X9 ...    X15    X16    X17  \\\n",
       "1          1   20000  2  2  1  24   2  2  -1  -1 ...      0      0      0   \n",
       "2          2  120000  2  2  2  26  -1  2   0   0 ...   3272   3455   3261   \n",
       "3          3   90000  2  2  2  34   0  0   0   0 ...  14331  14948  15549   \n",
       "4          4   50000  2  2  1  37   0  0   0   0 ...  28314  28959  29547   \n",
       "5          5   50000  1  2  1  57  -1  0  -1   0 ...  20940  19146  19131   \n",
       "\n",
       "    X18    X19    X20   X21   X22   X23  Y  \n",
       "1     0    689      0     0     0     0  1  \n",
       "2     0   1000   1000  1000     0  2000  1  \n",
       "3  1518   1500   1000  1000  1000  5000  0  \n",
       "4  2000   2019   1200  1100  1069  1000  0  \n",
       "5  2000  36681  10000  9000   689   679  0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data and remove the feature titles\n",
    "df = pd.read_csv('cc_clients.csv')\n",
    "data = df.drop(df.index[[0]])\n",
    "print('before: {}'.format(data.shape))\n",
    "data.drop_duplicates()\n",
    "print('after: {}'.format(data.shape))\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate the data into features and target\n",
    "training_data = np.asarray(data.loc[:, 'X1':'X23']).astype(np.float)\n",
    "target_data = np.asarray(data['Y']).astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Statistics to see how the dataset is transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal data distribution: Counter({0.0: 23364, 1.0: 6636})\n",
      "NearMiss(Undersampling) data distribution: Counter({0.0: 6636, 1.0: 6636})\n",
      "SMOTE(Oversampling) data distribution: Counter({1.0: 23364, 0.0: 23364})\n"
     ]
    }
   ],
   "source": [
    "# Results from over/undersampling vs. normal distribution\n",
    "print(\"Normal data distribution: {}\".format(Counter(target_data)))\n",
    "\n",
    "X_nearmiss, y_nearmiss = NearMiss().fit_sample(training_data, target_data)\n",
    "print(\"NearMiss(Undersampling) data distribution: {}\".format(Counter(y_nearmiss)))\n",
    "\n",
    "X_smote, y_smote = SMOTE().fit_sample(training_data, target_data)\n",
    "print(\"SMOTE(Oversampling) data distribution: {}\".format(Counter(y_smote)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare classification models(LR, SVM)\n",
    "classifier_lr = LogisticRegression\n",
    "classifier_svm = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NearMiss undersampling vs SMOTE oversampling(Logistic Regression and Support Vector Machines)<h4> - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normal model\n",
    "pipeline = make_pipeline(classifier_lr(random_state=42))\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nearmiss_pipeline = make_pipeline_imb(NearMiss(random_state=42), classifier_lr(random_state=42))\n",
    "nearmiss_model = nearmiss_pipeline.fit(X_train, y_train)\n",
    "nearmiss_prediction = nearmiss_model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(random_state=42), classifier_lr(random_state=42))\n",
    "smote_model = smote_pipeline.fit(X_train, y_train)\n",
    "smote_prediction = smote_model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing) \n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Score the models using the test data\n",
    "print('Logistic Regression accuracy, no class distribution {}'.format(pipeline.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, NearMiss undersampling {}'.format(nearmiss_pipeline.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, SMOTE oversampling {}'.format(smote_pipeline.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print_results(\"normal classification\", y_test, prediction)\n",
    "print()\n",
    "print_results(\"SMOTE classification\", y_test, smote_prediction)\n",
    "print()\n",
    "print_results(\"NearMiss classification\", y_test, nearmiss_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normal model\n",
    "pipeline = make_pipeline(classifier_svm)\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nearmiss_pipeline = make_pipeline_imb(NearMiss(random_state=42), classifier_svm)\n",
    "nearmiss_model = nearmiss_pipeline.fit(X_train, y_train)\n",
    "nearmiss_prediction = nearmiss_model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model = smote_pipeline.fit(X_train, y_train)\n",
    "smote_prediction = smote_model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Score the models using the test data\n",
    "print('SVM accuracy, no class distribution {}'.format(pipeline.score(X_test, y_test)))\n",
    "print('SVM accuracy, NearMiss undersampling {}'.format(nearmiss_pipeline.score(X_test, y_test)))\n",
    "print('SVM accuracy, SMOTE oversampling {}'.format(smote_pipeline.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Comparing performance of normal vs. SMOTE and NearMiss techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print_results(\"normal classification\", y_test, prediction)\n",
    "print()\n",
    "print_results(\"SMOTE classification\", y_test, smote_prediction)\n",
    "print()\n",
    "print_results(\"NearMiss classification\", y_test, nearmiss_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Rescaling and Normalisation (Logistic Regression and Support Vector Machines)<h3>Logistic Regression<h4> - Declaring scaling models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doing MinMax and Standard scaling and analysing results\n",
    "\n",
    "min_max = preprocessing.MinMaxScaler()\n",
    "training_minmax = min_max.fit_transform(training_data)\n",
    "\n",
    "std = preprocessing.StandardScaler()\n",
    "training_std = std.fit_transform(training_data)\n",
    "\n",
    "training_l1 = preprocessing.normalize(training_data, norm=\"l1\")\n",
    "\n",
    "training_l2 = preprocessing.normalize(training_data, norm=\"l2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Computing MSE for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE of Logistic Regression with MinMax, Scaling and Normalisation\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_data, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with nothing: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_minmax, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with MinMax: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_std, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with Standard Scaler: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_l1, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with Normalisation(L1): {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_l2, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with Normalisation(L2): {}\".format(mean_squared_error(y_test, prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # classification reports\n",
    "# print(classification_report(y_test, prediction))\n",
    "# # print(classification_report_imbalanced(y_test, nearmiss_prediction))\n",
    "# print(classification_report_imbalanced(y_test, smote_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Using scaling/normalisation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Logistic Regression with normal training data, MinMax and Standard scaling\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using Logistic Regression with training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "# print('LR training set accuracy(training data): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('Logistic Regression accuracy(training data): {}'.format(lr_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using Logistic Regression with MinMax scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "# print('Logistic Regression training set accuracy(Standard scaling): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('Logistic Regression accuracy(MinMax): {}'.format(lr_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using Logistic Regression with Standard scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "# print('Logistic Regression training set accuracy(Standard scaling): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('Logistic Regression accuracy(Standard scaling): {}'.format(lr_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using Logistic Regression with Normalisation(L1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "# print('LR training set accuracy(Normalisation, l1): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('Logistic Regression accuracy(Normalisation, l1): {}'.format(lr_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using Logistic Regression with Standard scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "# print('LR training set accuracy(Standard scaling): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('Logistic Regression accuracy(Normalisation, l2): {}'.format(lr_model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Adding sampling techniques to scaling/normalisation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling, Normalisation with class distribution techniques on Logistic Regression\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_minmax = make_pipeline_imb(SMOTE(), classifier_lr())\n",
    "smote_model_minmax = smote_pipeline_minmax.fit(X_train, y_train)\n",
    "smote_prediction_minmax = smote_model_minmax.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_std = make_pipeline_imb(SMOTE(), classifier_lr())\n",
    "smote_model_std = smote_pipeline_std.fit(X_train, y_train)\n",
    "smote_prediction_std = smote_model_std.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_l1 = make_pipeline_imb(SMOTE(), classifier_lr())\n",
    "smote_model_l1 = smote_pipeline_l1.fit(X_train, y_train)\n",
    "smote_prediction_l1 = smote_model_l1.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_l2 = make_pipeline_imb(SMOTE(), classifier_lr())\n",
    "smote_model_l2 = smote_pipeline_l2.fit(X_train, y_train)\n",
    "smote_prediction_l2 = smote_model_l2.predict(X_test)\n",
    "\n",
    "print('Logistic Regression accuracy, SMOTE, MinMax {}'.format(smote_pipeline_minmax.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, SMOTE, Standard scaling {}'.format(smote_pipeline_std.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, SMOTE, Normalisation(L1) {}'.format(smote_pipeline_l1.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, SMOTE, Normalisation(L2) {}'.format(smote_pipeline_l2.score(X_test, y_test)))\n",
    "\n",
    "print()\n",
    "#######################################################################################################################\n",
    "#######################################################################################################################\n",
    "\n",
    "# Scaling, Normalisation with class distribution techniques on Logistic Regression\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_minmax = make_pipeline_imb(NearMiss(), classifier_lr())\n",
    "nm_model_minmax = nm_pipeline_minmax.fit(X_train, y_train)\n",
    "nm_prediction_minmax = nm_model_minmax.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_std = make_pipeline_imb(NearMiss(), classifier_lr())\n",
    "nm_model_std = nm_pipeline_std.fit(X_train, y_train)\n",
    "nm_prediction_std = nm_model_std.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_l1 = make_pipeline_imb(NearMiss(), classifier_lr())\n",
    "nm_model_l1 = nm_pipeline_l1.fit(X_train, y_train)\n",
    "nm_prediction_l1 = nm_model_l1.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_l2 = make_pipeline_imb(NearMiss(), classifier_lr())\n",
    "nm_model_l2 = nm_pipeline_l2.fit(X_train, y_train)\n",
    "nm_prediction_l2 = nm_model_l2.predict(X_test)\n",
    "\n",
    "\n",
    "print('Logistic Regression accuracy, NearMiss, MinMax {}'.format(nm_pipeline_minmax.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, NearMiss, Standard scaling {}'.format(nm_pipeline_std.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, NearMiss, Normalisation(L1) {}'.format(nm_pipeline_l1.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, NearMiss, Normalisation(L2) {}'.format(nm_pipeline_l2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Support Vector Machine<h4> - Declaring scaling models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE of SVC with MinMax, Scaling and Normalisation\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_data, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with nothing: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_minmax, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with MinMax: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_std, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with Standard Scaler: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_l1, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with Normalisation(L1): {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_l2, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with Normalisation(L2): {}\".format(mean_squared_error(y_test, prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # classification reports\n",
    "# print(classification_report(y_test, prediction))\n",
    "# # print(classification_report_imbalanced(y_test, nearmiss_prediction))\n",
    "# print(classification_report_imbalanced(y_test, smote_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Using scaling/normalisation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SVMs with normal training data, MinMax and Standard scaling\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using SVMs with training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "# print('SVM training set accuracy(training data): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(training data): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "# Using SVMs with MinMax scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using SVMs on data\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "# print('SVM training set accuracy(Standard scaling): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(MinMax scaling): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "# Using SVMs with Standard scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using SVMs on data\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "# print('SVM training set accuracy(Standard scaling): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(Standard scaling): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "# Using SVMs with Normalisation(l1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using SVMs on data\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "# print('SVM training set accuracy(Normalisation, l1): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(Normalisation, l1): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "# Using SVMs with Normalisation(l2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using SVMs on data\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "# print('SVM training set accuracy(Normalisation, l1): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(Normalisation, l2): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - Adding sampling techniques to scaling/normalisation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling, Normalisation with class distribution techniques on Logistic Regression\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_minmax = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model_minmax = smote_pipeline_minmax.fit(X_train, y_train)\n",
    "smote_prediction_minmax = smote_model_minmax.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_std = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model_std = smote_pipeline_std.fit(X_train, y_train)\n",
    "smote_prediction_std = smote_model_std.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_l1 = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model_l1 = smote_pipeline_l1.fit(X_train, y_train)\n",
    "smote_prediction_l1 = smote_model_l1.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_l2 = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model_l2 = smote_pipeline_l2.fit(X_train, y_train)\n",
    "smote_prediction_l2 = smote_model_l2.predict(X_test)\n",
    "\n",
    "print('Support Vector Machine accuracy, SMOTE, MinMax {}'.format(smote_pipeline_minmax.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, SMOTE, Standard scaling {}'.format(smote_pipeline_std.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, SMOTE, Normalisation(L1) {}'.format(smote_pipeline_l1.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, SMOTE, Normalisation(L2) {}'.format(smote_pipeline_l2.score(X_test, y_test)))\n",
    "\n",
    "print()\n",
    "#######################################################################################################################\n",
    "#######################################################################################################################\n",
    "\n",
    "# Scaling, Normalisation with class distribution techniques on Logistic Regression\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_minmax = make_pipeline_imb(NearMiss(), classifier_svm)\n",
    "nm_model_minmax = nm_pipeline_minmax.fit(X_train, y_train)\n",
    "nm_prediction_minmax = nm_model_minmax.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_std = make_pipeline_imb(NearMiss(), classifier_svm)\n",
    "nm_model_std = nm_pipeline_std.fit(X_train, y_train)\n",
    "nm_prediction_std = nm_model_std.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_l1 = make_pipeline_imb(NearMiss(), classifier_svm)\n",
    "nm_model_l1 = nm_pipeline_l1.fit(X_train, y_train)\n",
    "nm_prediction_l1 = nm_model_l1.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline_l2 = make_pipeline_imb(NearMiss(), classifier_svm)\n",
    "nm_model_l2 = nm_pipeline_l2.fit(X_train, y_train)\n",
    "nm_prediction_l2 = nm_model_l2.predict(X_test)\n",
    "\n",
    "\n",
    "print('Support Vector Machine accuracy, NearMiss, MinMax {}'.format(nm_pipeline_minmax.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, NearMiss, Standard scaling {}'.format(nm_pipeline_std.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, NearMiss, Normalisation(L1) {}'.format(nm_pipeline_l1.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, NearMiss, Normalisation(L2) {}'.format(nm_pipeline_l2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature Selection (SelectPercentile, SelectFromModel)<h4> - SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SelectPercentile technique on Logistic Regression and scaling methods used previously\n",
    "sp_model = SelectPercentile(percentile=40)\n",
    "\n",
    "#####################################################################################################################\n",
    "# Logistic Regression, Training data, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "sp_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, using training data: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), lr_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, NearMiss, using training data: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), lr_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, SMOTE, using training data: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "########################################################\n",
    "# SVC\n",
    "print()\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, using training data: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), svm_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, NearMiss, using training data: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), svm_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, SMOTE, using training data: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "######################################################################################################################\n",
    "print()\n",
    "print()\n",
    "# Logistic Regression, Training data, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "sp_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, MinMax: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), lr_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, NearMiss, MinMax: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), lr_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, SMOTE, MinMax: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "########################################################\n",
    "# SVC\n",
    "print()\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, MinMax: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), svm_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, NearMiss, MinMax: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), svm_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, SMOTE, MinMax: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Logistic Regression, Training data, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "sp_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, Standard Scaling: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), lr_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, NearMiss, Standard Scaling: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), lr_model)\n",
    "# smote_model = smote_pipeline.fit(X_train_selected, y_train)\n",
    "# smote_prediction = smote_model.predict(X_test_selected)\n",
    "print('Logistic Regression accuracy, SelectPercentile, SMOTE, Standard Scaling: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "########################################################\n",
    "# SVC\n",
    "print()\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, Standard Scaling: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), svm_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, NearMiss, Standard Scaling: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), svm_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, SMOTE, Standard Scaling: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Logistic Regression, Training data, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "sp_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, Normalisation(L1): {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), lr_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, NearMiss, Normalisation(L1): {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), lr_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectPercentile, SMOTE, Normalisation(L1): {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "########################################################\n",
    "# SVC\n",
    "print()\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, Normalisation(L1): {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), svm_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, NearMiss, Normalisation(L1): {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), svm_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectPercentile, SMOTE, Normalisation(L1): {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> - SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SelectFromModel technique on Logistic Regression and scaling methods used previously\n",
    "sm_model = SelectFromModel(LinearSVC(C=0.01, penalty='l1', dual=False))\n",
    "# sm_model = SelectFromModel(LassoCV())\n",
    "\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "sm_model.fit(X_train, y_train)\n",
    "X_train_selected = sm_model.transform(X_train)\n",
    "X_test_selected = sm_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, using training data: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), lr_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, NearMiss, using training data: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), lr_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, SMOTE, using training data: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "########################################################\n",
    "# SVC\n",
    "print()\n",
    "svm_model = SVC()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "sm_model.fit(X_train, y_train)\n",
    "X_train_selected = sm_model.transform(X_train)\n",
    "X_test_selected = sm_model.transform(X_test)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), svm_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, NearMiss, using training data: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), svm_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, SMOTE, using training data: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "print('')\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "sm_model.fit(X_train, y_train)\n",
    "X_train_selected = sm_model.transform(X_train)\n",
    "X_test_selected = sm_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, MinMax: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), lr_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, NearMiss, MinMax: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), lr_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, SMOTE, MinMax: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "########################################################\n",
    "# SVC\n",
    "print()\n",
    "svm_model = SVC()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "sm_model.fit(X_train, y_train)\n",
    "X_train_selected = sm_model.transform(X_train)\n",
    "X_test_selected = sm_model.transform(X_test)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, MinMax: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), svm_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, NearMiss, MinMax: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), svm_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, SMOTE, MinMax: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "print('')\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "sm_model.fit(X_train, y_train)\n",
    "X_train_selected = sm_model.transform(X_train)\n",
    "X_test_selected = sm_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, Standard scaling: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), lr_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, NearMiss, Standard scaling: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), lr_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, SMOTE, Standard scaling: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "########################################################\n",
    "# SVC\n",
    "print()\n",
    "svm_model = SVC()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "sm_model.fit(X_train, y_train)\n",
    "X_train_selected = sm_model.transform(X_train)\n",
    "X_test_selected = sm_model.transform(X_test)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, Standard scaling: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), svm_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, NearMiss, Standard scaling: {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), svm_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, SMOTE, Standard scaling: {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "print('')\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "sm_model.fit(X_train, y_train)\n",
    "X_train_selected = sm_model.transform(X_train)\n",
    "X_test_selected = sm_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, (Normalisation, l1): {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), lr_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, NearMiss, (Normalisation, l1): {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), lr_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Logistic Regression accuracy, SelectFromModel, SMOTE, (Normalisation, l1): {}'.format(smote_pipeline.score(X_test_selected, y_test)))\n",
    "########################################################\n",
    "# SVC\n",
    "print()\n",
    "svm_model = SVC()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "sm_model.fit(X_train, y_train)\n",
    "X_train_selected = sm_model.transform(X_train)\n",
    "X_test_selected = sm_model.transform(X_test)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, (Normalisation, l1): {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nm_pipeline = make_pipeline_imb(NearMiss(), svm_model)\n",
    "nm_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, NearMiss, (Normalisation, l1): {}'.format(nm_pipeline.score(X_test_selected, y_test)))\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), svm_model)\n",
    "smote_pipeline.fit(X_train_selected, y_train)\n",
    "print('Support Vector Machine accuracy, SelectFromModel, SMOTE, (Normalisation, l1): {}'.format(smote_pipeline.score(X_test_selected, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> PCA Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines\n",
    "estimators = [('reduce_dim', PCA(n_components=10)), ('clf', SVC())]\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train) \n",
    "print('RBF Kernel, MinMax scaling, PCA(10 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('RBF Kernel, Standard scaling, PCA(10 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('RBF Kernel, Normalisation(L1), PCA(10 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "estimators = [('reduce_dim', PCA(n_components=10)), ('clf', SVC(kernel='linear'))]\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Linear Kernel, MinMax scaling, PCA(10 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Linear Kernel, Standard scaling, PCA(10 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Linear Kernel, Normalisation(L1), PCA(10 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "estimators = [('reduce_dim', PCA(n_components=5)), ('clf', SVC())]\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('RBF Kernel, MinMax scaling, PCA(5 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('RBF Kernel, Standard scaling, PCA(5 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('RBF Kernel, Normalisation(L1), PCA(5 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "estimators = [('reduce_dim', PCA(n_components=5)), ('clf', SVC(kernel='linear'))]\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Linear Kernel, MinMax scaling, PCA(5 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Linear Kernel, Standard scaling, PCA(5 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Linear Kernel, Normalisation(L1), PCA(5 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "\n",
    "pca = PCA(.95)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pca.fit(X_train)\n",
    "print('Retaining 95% variance with {} components'.format(pca.n_components_))\n",
    "########################################################\n",
    "########################################################\n",
    "print('')\n",
    "pca = PCA(.85)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pca.fit(X_train)\n",
    "print('Retaining 85% variance with {} components'.format(pca.n_components_))\n",
    "print('')\n",
    "######################################################################################################################\n",
    "estimators = [('reduce_dim', PCA(n_components=3)), ('clf', SVC())]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('RBF Kernel, MinMax scaling, PCA(3 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('RBF Kernel, Standard scaling scaling, PCA(3 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('RBF Kernel, Normalisation(L1), PCA(3 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "#####################################################################################################################\n",
    "estimators = [('reduce_dim', PCA(n_components=3)), ('clf', SVC(kernel='linear'))]\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Linear Kernel, MinMax scaling, PCA(3 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Linear Kernel, Standard scaling, PCA(3 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Linear Kernel, Normalisation(L1), PCA(3 dims): {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy, 10-fold cross-validation: 0.810 (+/- (0.014))\n"
     ]
    }
   ],
   "source": [
    "# 10-Fold Cross validation\n",
    "logreg = LogisticRegression()\n",
    "scores = cross_val_score(logreg, training_std, target_data, cv=10)\n",
    "print('Logistic regression accuracy, 10-fold cross-validation: %0.3f (+/- (%.3f))' % (scores.mean(), scores.std()*2))\n",
    "\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "scores = cross_val_score(svm, training_std, target_data, cv=10)\n",
    "print('Linear kernel SVM accuracy, 10-fold cross-validation: %0.3f (+/- (%.3f))' % (scores.mean(), scores.std()*2))\n",
    "\n",
    "svm = SVC()\n",
    "scores = cross_val_score(svm, training_std, target_data, cv=10)\n",
    "print('RBF Kernel accuracy, 10-fold cross-validation: %0.3f (+/- (%.3f))' % (scores.mean(), scores.std()*2))\n",
    "\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "print()\n",
    "print()\n",
    "logreg = LogisticRegression()\n",
    "scores = cross_val_score(logreg, training_std, target_data, cv=10, scoring='recall')\n",
    "print('Logistic regression recall, 10-fold cross-validation: %0.3f)' % (scores.mean()))\n",
    "\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "scores = cross_val_score(svm, training_std, target_data, cv=10, scoring='recall')\n",
    "print('Linear kernel SVM recall, 10-fold cross-validation: %0.3f)' % (scores.mean()))\n",
    "\n",
    "svm = SVC()\n",
    "scores = cross_val_score(svm, training_std, target_data, cv=10, scoring='recall')\n",
    "print('RBF Kernel recall, 10-fold cross-validation: %0.3f)' % (scores.mean()))\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "print()\n",
    "print()\n",
    "logreg = LogisticRegression()\n",
    "scores = cross_val_score(logreg, training_std, target_data, cv=10, scoring='precision')\n",
    "print('Logistic regression precision, 10-fold cross-validation: %0.3f)' % (scores.mean()))\n",
    "\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "scores = cross_val_score(svm, training_std, target_data, cv=10, scoring='precision')\n",
    "print('Linear kernel SVM precision, 10-fold cross-validation: %0.3f)' % (scores.mean()))\n",
    "\n",
    "svm = SVC()\n",
    "scores = cross_val_score(svm, training_std, target_data, cv=10, scoring='precision')\n",
    "print('RBF Kernel precision, 10-fold cross-validation: %0.3f)' % (scores.mean()))\n",
    "#####################################################################################################################\n",
    "print()\n",
    "print()\n",
    "logreg = LogisticRegression()\n",
    "scores = cross_val_score(logreg, training_std, target_data, cv=10, scoring='f1')\n",
    "print('Logistic regression F1 score, 10-fold cross-validation: %0.3f)' % (scores.mean()))\n",
    "\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "scores = cross_val_score(svm, training_std, target_data, cv=10, scoring='f1')\n",
    "print('Linear kernel SVM F1 score, 10-fold cross-validation: %0.3f)' % (scores.mean()))\n",
    "\n",
    "svm = SVC()\n",
    "scores = cross_val_score(svm, training_std, target_data, cv=10, scoring='f1')\n",
    "print('RBF Kernel F1 score, 10-fold cross-validation: %0.3f)' % (scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Printing mean of several metrics after 5-fold cross validation\n",
    "# print(\"Mean of scores 5-fold:\")\n",
    "# print(\"Accuracy: {}\".format(np.mean(accuracy)))\n",
    "# print(\"Precision: {}\".format(np.mean(precision)))\n",
    "# print(\"Recall: {}\".format(np.mean(recall)))\n",
    "# print(\"F1: {}\".format(np.mean(f1)))\n",
    "# print(\"Auc: {}\".format(np.mean(auc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper-parameter tuning\n",
    "#####################################################################################################################\n",
    "\n",
    "# GridSearch\n",
    "# tuned_parameters = [\n",
    "#     {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [0.01, 0.1, 1]},\n",
    "#     {'kernel': ['linear'], 'C': [0.01, 0.1, 1]}\n",
    "# ]\n",
    "\n",
    "# clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=10, scoring='f1_macro')\n",
    "\n",
    "# print('Training data')\n",
    "# X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "# clf.fit(X_train, y_train)\n",
    "# clf.best_params_\n",
    "# y_true, y_pred = y_test, clf.predict(X_test)\n",
    "\n",
    "# print('Standard scaling')\n",
    "# X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "# clf.fit(X_train, y_train)\n",
    "# clf.best_params_\n",
    "# y_true, y_pred = y_test, clf.predict(X_test)\n",
    "\n",
    "# print('Normalisation L1')\n",
    "# X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "# clf.fit(X_train, y_train)\n",
    "# clf.best_params_\n",
    "# y_true, y_pred = y_test, clf.predict(X_test)\n",
    "\n",
    "\n",
    "# print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
