{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scoring metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# model selection stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import scipy\n",
    "# scaling and normalisation metrics\n",
    "from sklearn import preprocessing\n",
    "# classifier models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.svm import SVC\n",
    "# graphical stuff\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# feature selection\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "# dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# helper function to print the data\n",
    "def print_results(headline, true_value, pred):\n",
    "    print(headline)\n",
    "    print(\"accuracy: {}\".format(accuracy_score(true_value, pred)))\n",
    "    print(\"precision: {}\".format(precision_score(true_value, pred)))\n",
    "    print(\"recall: {}\".format(recall_score(true_value, pred)))\n",
    "    print(\"f1: {}\".format(f1_score(true_value, pred)))\n",
    "\n",
    "# helper function to build the model\n",
    "def build_lr_model_for_data(data, target):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42, test_size=0.2)\n",
    "    pipeline = make_pipeline(LogisticRegression())\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    return (X_test, y_test, model)\n",
    "\n",
    "# helper function to build the model\n",
    "def build_svc_model_for_data(data, target):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42, test_size=0.2)\n",
    "    pipeline = make_pipeline(SVC())\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    return (X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: (30000, 25)\n",
      "after: (30000, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0      X1 X2 X3 X4  X5  X6 X7  X8  X9 ...    X15    X16    X17  \\\n",
       "1          1   20000  2  2  1  24   2  2  -1  -1 ...      0      0      0   \n",
       "2          2  120000  2  2  2  26  -1  2   0   0 ...   3272   3455   3261   \n",
       "3          3   90000  2  2  2  34   0  0   0   0 ...  14331  14948  15549   \n",
       "4          4   50000  2  2  1  37   0  0   0   0 ...  28314  28959  29547   \n",
       "5          5   50000  1  2  1  57  -1  0  -1   0 ...  20940  19146  19131   \n",
       "\n",
       "    X18    X19    X20   X21   X22   X23  Y  \n",
       "1     0    689      0     0     0     0  1  \n",
       "2     0   1000   1000  1000     0  2000  1  \n",
       "3  1518   1500   1000  1000  1000  5000  0  \n",
       "4  2000   2019   1200  1100  1069  1000  0  \n",
       "5  2000  36681  10000  9000   689   679  0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data and remove the feature titles\n",
    "df = pd.read_csv('cc_clients.csv')\n",
    "data = df.drop(df.index[[0]])\n",
    "print('before: {}'.format(data.shape))\n",
    "data.drop_duplicates()\n",
    "print('after: {}'.format(data.shape))\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate the data into features and target\n",
    "training_data = np.asarray(data.loc[:, 'X1':'X23']).astype(np.float)\n",
    "target_data = np.asarray(data['Y']).astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Statistics to see how the dataset is transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal data distribution: Counter({0.0: 23364, 1.0: 6636})\n",
      "NearMiss(Undersampling) data distribution: Counter({0.0: 6636, 1.0: 6636})\n",
      "SMOTE(Oversampling) data distribution: Counter({1.0: 23364, 0.0: 23364})\n"
     ]
    }
   ],
   "source": [
    "# Results from over/undersampling vs. normal distribution\n",
    "print(\"Normal data distribution: {}\".format(Counter(target_data)))\n",
    "\n",
    "X_nearmiss, y_nearmiss = NearMiss().fit_sample(training_data, target_data)\n",
    "print(\"NearMiss(Undersampling) data distribution: {}\".format(Counter(y_nearmiss)))\n",
    "\n",
    "X_smote, y_smote = SMOTE().fit_sample(training_data, target_data)\n",
    "print(\"SMOTE(Oversampling) data distribution: {}\".format(Counter(y_smote)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare classification models(LR, SVM)\n",
    "classifier_lr = LogisticRegression\n",
    "classifier_svm = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NearMiss undersampling vs SMOTE oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy, no class distribution 0.7811666666666667\n",
      "Logistic Regression accuracy, NearMiss undersampling 0.336\n",
      "Logistic Regression accuracy, SMOTE oversampling 0.5586666666666666\n"
     ]
    }
   ],
   "source": [
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normal model\n",
    "pipeline = make_pipeline(classifier_lr())\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nearmiss_pipeline = make_pipeline_imb(NearMiss(), classifier_lr(random_state=42))\n",
    "nearmiss_model = nearmiss_pipeline.fit(X_train, y_train)\n",
    "nearmiss_prediction = nearmiss_model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), classifier_lr(random_state=42))\n",
    "smote_model = smote_pipeline.fit(X_train, y_train)\n",
    "smote_prediction = smote_model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing) \n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Score the models using the test data\n",
    "print('Logistic Regression accuracy, no class distribution {}'.format(pipeline.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, NearMiss undersampling {}'.format(nearmiss_pipeline.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, SMOTE oversampling {}'.format(smote_pipeline.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normal model\n",
    "pipeline = make_pipeline(classifier_svm)\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "nearmiss_pipeline = make_pipeline_imb(NearMiss(random_state=42), classifier_svm)\n",
    "nearmiss_model = nearmiss_pipeline.fit(X_train, y_train)\n",
    "nearmiss_prediction = nearmiss_model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model = smote_pipeline.fit(X_train, y_train)\n",
    "smote_prediction = smote_model.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Score the models using the test data\n",
    "print('SVM accuracy, no class distribution {}'.format(pipeline.score(X_test, y_test)))\n",
    "print('SVM accuracy, NearMiss undersampling {}'.format(nearmiss_pipeline.score(X_test, y_test)))\n",
    "print('SVM accuracy, SMOTE oversampling {}'.format(smote_pipeline.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal classification\n",
      "accuracy: 0.7811666666666667\n",
      "precision: 0.5\n",
      "recall: 0.0007616146230007616\n",
      "f1: 0.0015209125475285172\n",
      "\n",
      "SMOTE classification\n",
      "accuracy: 0.5586666666666666\n",
      "precision: 0.2956841138659321\n",
      "recall: 0.7357197258187357\n",
      "f1: 0.4218340611353712\n",
      "\n",
      "NearMiss classification\n",
      "accuracy: 0.336\n",
      "precision: 0.18134096874254355\n",
      "recall: 0.5788271134805788\n",
      "f1: 0.2761627906976744\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print_results(\"normal classification\", y_test, prediction)\n",
    "print()\n",
    "print_results(\"SMOTE classification\", y_test, smote_prediction)\n",
    "print()\n",
    "print_results(\"NearMiss classification\", y_test, nearmiss_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Rescaling and Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Declaring scaling instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing MinMax and Standard scaling and analysing results\n",
    "\n",
    "min_max = preprocessing.MinMaxScaler()\n",
    "training_minmax = min_max.fit_transform(training_data)\n",
    "\n",
    "std = preprocessing.StandardScaler()\n",
    "training_std = std.fit_transform(training_data)\n",
    "\n",
    "training_l1 = preprocessing.normalize(training_data, norm=\"l1\")\n",
    "\n",
    "training_l2 = preprocessing.normalize(training_data, norm=\"l2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Computing MSE for Logistic Regression using several scaling and normalisation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Logistic Regression with nothing: 0.21883333333333332\n",
      "MSE of Logistic Regression with MinMax: 0.19\n",
      "MSE of Logistic Regression with Standard Scaler: 0.19016666666666668\n",
      "MSE of Logistic Regression with Normalisation(L1): 0.21883333333333332\n",
      "MSE of Logistic Regression with Normalisation(L2): 0.21916666666666668\n"
     ]
    }
   ],
   "source": [
    "# MSE of Logistic Regression with MinMax, Scaling and Normalisation\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_data, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with nothing: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_minmax, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with MinMax: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_std, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with Standard Scaler: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_l1, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with Normalisation(L1): {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_lr_model_for_data(training_l2, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Logistic Regression with Normalisation(L2): {}\".format(mean_squared_error(y_test, prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Using scaling/normalisation techniques with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR test set accuracy(training data): 0.7788333333333334\n",
      "\n",
      "LR test set accuracy(MinMax): 0.8075\n",
      "\n",
      "LR test set accuracy(Standard scaling): 0.8078333333333333\n",
      "\n",
      "LR test set accuracy(Normalisation, l1): 0.7788333333333334\n",
      "\n",
      "LR test set accuracy(Normalisation, l2): 0.7788333333333334\n"
     ]
    }
   ],
   "source": [
    "# Using LR with normal training data, MinMax and Standard scaling\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using LR with training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "# print('LR training set accuracy(training data): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('LR test set accuracy(training data): {}'.format(lr_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using LR with MinMax scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "print('')\n",
    "# print('LR training set accuracy(Standard scaling): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('LR test set accuracy(MinMax): {}'.format(lr_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using LR with Standard scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "print('')\n",
    "# print('LR training set accuracy(Standard scaling): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('LR test set accuracy(Standard scaling): {}'.format(lr_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using LR with Normalisation(L1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "print('')\n",
    "# print('LR training set accuracy(Normalisation, l1): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('LR test set accuracy(Normalisation, l1): {}'.format(lr_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using LR with Standard scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using Logistic Regression on data\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "\n",
    "print('')\n",
    "# print('LR training set accuracy(Standard scaling): {}'.format(lr_model.score(X_train, y_train)))\n",
    "print('LR test set accuracy(Normalisation, l2): {}'.format(lr_model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adding sampling techniques to scaling/normalisation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy, SMOTE, MinMax 0.7811666666666667\n",
      "Logistic Regression accuracy, SMOTE, Standard scaling 0.7811666666666667\n",
      "Logistic Regression accuracy, SMOTE, Normalisation(L1) 0.45466666666666666\n",
      "Logistic Regression accuracy, SMOTE, Normalisation(L2) 0.5921666666666666\n",
      "\n",
      "Logistic Regression accuracy, NearMiss, MinMax 0.3978333333333333\n",
      "Logistic Regression accuracy, NearMiss, Standard scaling 0.21883333333333332\n",
      "Logistic Regression accuracy, NearMiss, Normalisation(L1) 0.7408333333333333\n",
      "Logistic Regression accuracy, NearMiss, Normalisation(L2) 0.3695\n"
     ]
    }
   ],
   "source": [
    "# Scaling, Normalisation with class distribution techniques on Logistic Regression\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_minmax = make_pipeline_imb(SMOTE(), classifier_lr())\n",
    "smote_model_minmax = smote_pipeline_minmax.fit(X_train, y_train)\n",
    "smote_prediction_minmax = smote_model_minmax.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_std = make_pipeline_imb(SMOTE(), classifier_lr())\n",
    "smote_model_std = smote_pipeline_std.fit(X_train, y_train)\n",
    "smote_prediction_std = smote_model_std.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_l1 = make_pipeline_imb(SMOTE(), classifier_lr())\n",
    "smote_model_l1 = smote_pipeline_l1.fit(X_train, y_train)\n",
    "smote_prediction_l1 = smote_model_l1.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_l2 = make_pipeline_imb(SMOTE(), classifier_lr())\n",
    "smote_model_l2 = smote_pipeline_l2.fit(X_train, y_train)\n",
    "smote_prediction_l2 = smote_model_l2.predict(X_test)\n",
    "\n",
    "print('Logistic Regression accuracy, SMOTE, MinMax {}'.format(smote_pipeline_minmax.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, SMOTE, Standard scaling {}'.format(smote_pipeline_std.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, SMOTE, Normalisation(L1) {}'.format(smote_pipeline_l1.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, SMOTE, Normalisation(L2) {}'.format(smote_pipeline_l2.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "print()\n",
    "#######################################################################################################################\n",
    "#######################################################################################################################\n",
    "\n",
    "# Scaling, Normalisation with class distribution techniques on Logistic Regression\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "smote_pipeline_minmax = make_pipeline_imb(NearMiss(), classifier_lr())\n",
    "smote_model_minmax = smote_pipeline_minmax.fit(X_train, y_train)\n",
    "smote_prediction_minmax = smote_model_minmax.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "smote_pipeline_std = make_pipeline_imb(NearMiss(), classifier_lr())\n",
    "smote_model_std = smote_pipeline_std.fit(X_train, y_train)\n",
    "smote_prediction_std = smote_model_std.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "smote_pipeline_l1 = make_pipeline_imb(NearMiss(), classifier_lr())\n",
    "smote_model_l1 = smote_pipeline_l1.fit(X_train, y_train)\n",
    "smote_prediction_l1 = smote_model_l1.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "smote_pipeline_l2 = make_pipeline_imb(NearMiss(), classifier_lr())\n",
    "smote_model_l2 = smote_pipeline_l2.fit(X_train, y_train)\n",
    "smote_prediction_l2 = smote_model_l2.predict(X_test)\n",
    "\n",
    "\n",
    "print('Logistic Regression accuracy, NearMiss, MinMax {}'.format(smote_pipeline_minmax.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, NearMiss, Standard scaling {}'.format(smote_pipeline_std.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, NearMiss, Normalisation(L1) {}'.format(smote_pipeline_l1.score(X_test, y_test)))\n",
    "print('Logistic Regression accuracy, NearMiss, Normalisation(L2) {}'.format(smote_pipeline_l2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Support Vector Machine with nothing: 0.21933333333333332\n",
      "MSE of Support Vector Machine with MinMax: 0.212\n",
      "MSE of Support Vector Machine with Standard Scaler: 0.18066666666666667\n",
      "MSE of Support Vector Machine with Normalisation(L1): 0.21883333333333332\n",
      "MSE of Support Vector Machine with Normalisation(L2): 0.21883333333333332\n"
     ]
    }
   ],
   "source": [
    "# MSE of SVC with MinMax, Scaling and Normalisation\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_data, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with nothing: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_minmax, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with MinMax: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_std, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with Standard Scaler: {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_l1, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with Normalisation(L1): {}\".format(mean_squared_error(y_test, prediction)))\n",
    "\n",
    "X_test, y_test, model = build_svc_model_for_data(training_l2, target_data)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"MSE of Support Vector Machine with Normalisation(L2): {}\".format(mean_squared_error(y_test, prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification reports\n",
    "print(classification_report(y_test, prediction))\n",
    "# print(classification_report_imbalanced(y_test, nearmiss_prediction))\n",
    "print(classification_report_imbalanced(y_test, smote_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_model100 = LogisticRegression(C=0.01)\n",
    "# lr_model100 = lr_model.fit(X_train, y_train)\n",
    "\n",
    "# print('Accuracy on the training set, using Logistic Regression: {}'.format(lr_model100.score(X_train, y_train)))\n",
    "# print('Accuracy on the test set, using Logistic Regression: {}'.format(lr_model100.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Using scaling/normalisation techniques with Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM test set accuracy(training data): 0.7791666666666667\n",
      "\n",
      "SVM test set accuracy(Standard scaling): 0.8158333333333333\n",
      "\n",
      "SVM test set accuracy(Normalisation, l1): 0.7788333333333334\n",
      "\n",
      "SVM test set accuracy(Normalisation, l2): 0.7788333333333334\n"
     ]
    }
   ],
   "source": [
    "# Using SVMs with normal training data, MinMax and Standard scaling\n",
    "#####################################################################################################################\n",
    "\n",
    "# Using SVMs with training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "# print('SVM training set accuracy(training data): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(training data): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "# Using SVMs with Standard scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using SVMs on data\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "print('')\n",
    "# print('SVM training set accuracy(Standard scaling): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(Standard scaling): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "# Using SVMs with Normalisation(l1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using SVMs on data\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "print('')\n",
    "# print('SVM training set accuracy(Normalisation, l1): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(Normalisation, l1): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "# Using SVMs with Normalisation(l2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "# Using SVMs on data\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "\n",
    "print('')\n",
    "# print('SVM training set accuracy(Normalisation, l1): {}'.format(svm_model.score(X_train, y_train)))\n",
    "print('SVM test set accuracy(Normalisation, l2): {}'.format(svm_model.score(X_test, y_test)))\n",
    "#####################################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adding sampling techniques to scaling/normalisation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine accuracy, SMOTE, MinMax 0.588\n",
      "Support Vector Machine accuracy, SMOTE, Standard scaling 0.588\n",
      "Support Vector Machine accuracy, SMOTE, Normalisation(L1) 0.588\n",
      "Support Vector Machine accuracy, SMOTE, Normalisation(L2) 0.588\n",
      "\n",
      "Support Vector Machine accuracy, NearMiss, MinMax 0.44\n",
      "Support Vector Machine accuracy, NearMiss, Standard scaling 0.44\n",
      "Support Vector Machine accuracy, NearMiss, Normalisation(L1) 0.44\n",
      "Support Vector Machine accuracy, NearMiss, Normalisation(L2) 0.44\n"
     ]
    }
   ],
   "source": [
    "# Scaling, Normalisation with class distribution techniques on Logistic Regression\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_minmax = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model_minmax = smote_pipeline_minmax.fit(X_train, y_train)\n",
    "smote_prediction_minmax = smote_model_minmax.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_std = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model_std = smote_pipeline_std.fit(X_train, y_train)\n",
    "smote_prediction_std = smote_model_std.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_l1 = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model_l1 = smote_pipeline_l1.fit(X_train, y_train)\n",
    "smote_prediction_l1 = smote_model_l1.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Minority oversampling(SMOTE)\n",
    "smote_pipeline_l2 = make_pipeline_imb(SMOTE(), classifier_svm)\n",
    "smote_model_l2 = smote_pipeline_l2.fit(X_train, y_train)\n",
    "smote_prediction_l2 = smote_model_l2.predict(X_test)\n",
    "\n",
    "print('Support Vector Machine accuracy, SMOTE, MinMax {}'.format(smote_pipeline_minmax.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, SMOTE, Standard scaling {}'.format(smote_pipeline_std.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, SMOTE, Normalisation(L1) {}'.format(smote_pipeline_l1.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, SMOTE, Normalisation(L2) {}'.format(smote_pipeline_l2.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "print()\n",
    "#######################################################################################################################\n",
    "#######################################################################################################################\n",
    "\n",
    "# Scaling, Normalisation with class distribution techniques on Logistic Regression\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "smote_pipeline_minmax = make_pipeline_imb(NearMiss(), classifier_svm)\n",
    "smote_model_minmax = smote_pipeline_minmax.fit(X_train, y_train)\n",
    "smote_prediction_minmax = smote_model_minmax.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "smote_pipeline_std = make_pipeline_imb(NearMiss(), classifier_svm)\n",
    "smote_model_std = smote_pipeline_std.fit(X_train, y_train)\n",
    "smote_prediction_std = smote_model_std.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "smote_pipeline_l1 = make_pipeline_imb(NearMiss(), classifier_svm)\n",
    "smote_model_l1 = smote_pipeline_l1.fit(X_train, y_train)\n",
    "smote_prediction_l1 = smote_model_l1.predict(X_test)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# Split the data into 80/20(training/testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l2, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Majority undersampling(NearMiss)\n",
    "smote_pipeline_l2 = make_pipeline_imb(NearMiss(), classifier_svm)\n",
    "smote_model_l2 = smote_pipeline_l2.fit(X_train, y_train)\n",
    "smote_prediction_l2 = smote_model_l2.predict(X_test)\n",
    "\n",
    "\n",
    "print('Support Vector Machine accuracy, NearMiss, MinMax {}'.format(smote_pipeline_minmax.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, NearMiss, Standard scaling {}'.format(smote_pipeline_std.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, NearMiss, Normalisation(L1) {}'.format(smote_pipeline_l1.score(X_test, y_test)))\n",
    "print('Support Vector Machine accuracy, NearMiss, Normalisation(L2) {}'.format(smote_pipeline_l2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SelectPercentile technique on Logistic Regression and scaling methods used previously\n",
    "sp_model = SelectPercentile(percentile=40)\n",
    "\n",
    "#####################################################################################################################\n",
    "# LR, Training data, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "print('LR with all features: {}'.format(lr_model.score(X_test, y_test)))\n",
    "\n",
    "sp_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('LR with feature selection[SelectPercentile]: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C = 1.0)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('SVM with feature selection[SelectPercentile]: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "\n",
    "# LR, Standard scaling, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "print('LR with all features(Standard scaling): {}'.format(lr_model.score(X_test, y_test)))\n",
    "\n",
    "sp_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('LR with feature selection(Standard scaling)[SelectPercentile]: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C = 1.0)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('SVM with feature selection(Standard scaling)[SelectPercentile]: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "# LR, L1 normalisation, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "  \n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "print('LR with all features(Normalisation, l1): {}'.format(lr_model.score(X_test, y_test)))\n",
    "\n",
    "sp_model.fit(X_train, y_train)\n",
    "X_train_selected = sp_model.transform(X_train)\n",
    "X_test_selected = sp_model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('LR with feature selection(Normalisation, l1)[SelectPercentile]: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C = 1.0)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('SVM with feature selection(Normalisation, l1)[SelectPercentile]: {}'.format(svm_model.score(X_test_selected, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SelectFromModel technique on Logistic Regression and scaling methods used previously\n",
    "model = SelectFromModel(LogisticRegression(C=0.01, dual=False))\n",
    "#####################################################################################################################\n",
    "# LR, Training data, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "print('LR with all features: {}'.format(lr_model.score(X_test, y_test)))\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "X_train_selected = model.transform(X_train)\n",
    "X_test_selected = model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('LR with feature selection[SelectFromModel]: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C = 1.0)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('SVM with feature selection[SelectPercentile]: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "\n",
    "# LR, Standard scaling, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "print('LR with all features(Standard scaling): {}'.format(lr_model.score(X_test, y_test)))\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "X_train_selected = model.transform(X_train)\n",
    "X_test_selected = model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('LR with feature selection(Standard scaling)[SelectFromModel]: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C = 1.0)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('SVM with feature selection(Standard scaling)[SelectPercentile]: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "\n",
    "# LR, L1 normalisation, Feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model = lr_model.fit(X_train, y_train)\n",
    "print('LR with all features(Normalisation, l1): {}'.format(lr_model.score(X_test, y_test)))\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "X_train_selected = model.transform(X_train)\n",
    "X_test_selected = model.transform(X_test)\n",
    "lr_model.fit(X_train_selected, y_train)\n",
    "print('LR with feature selection(Normalisation, l1)[SelectFromModel]: {}'.format(lr_model.score(X_test_selected, y_test)))\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C = 1.0)\n",
    "svm_model.fit(X_train_selected, y_train)\n",
    "print('SVM with feature selection(Normalisation, l1)[SelectPercentile]: {}'.format(svm_model.score(X_test_selected, y_test)))\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> PCA Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines\n",
    "estimators = [('reduce_dim', PCA(n_components=10)), ('clf', SVC())]\n",
    "\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 10 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 10 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 10 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 10 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "estimators = [('reduce_dim', PCA(n_components=10, whiten=True)), ('clf', SVC())]\n",
    "\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 10 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 10 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 10 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 10 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "\n",
    "#####################################################################################################################\n",
    "print('')\n",
    "estimators = [('reduce_dim', PCA(n_components=5)), ('clf', SVC())]\n",
    "\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 5 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_minmax, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 5 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 5 components: {}'.format(pipe.score(X_test, y_test)))\n",
    "#####################################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "pipe.fit(X_train, y_train)\n",
    "print('PCA with 5 components: {}'.format(pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter tuning\n",
    "#####################################################################################################################\n",
    "\n",
    "# GridSearch\n",
    "tuned_parameters = [\n",
    "    {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [0.01, 0.1, 1]},\n",
    "    {'kernel': ['linear'], 'C': [0.01, 0.1, 1]}\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=10, scoring='f1_macro')\n",
    "\n",
    "print('Training data')\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.best_params_\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "\n",
    "print('Standard scaling')\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_std, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.best_params_\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "\n",
    "print('Normalisation L1')\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_l1, target_data, stratify=target_data, random_state=42, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.best_params_\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model evaluation(chateau)\n",
    "\n",
    "# GridSearchCV and cross_val_score take different scoring parameters\n",
    "# classification report\n",
    "# confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> To do next..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feat evaluation\n",
    "\n",
    "# add SMOTE to experiments\n",
    "# cross validation\n",
    "# visualisation(scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "logreg = LogisticRegression()\n",
    "print('Logistic regression cross-validation accuracy: %0.4f' % cross_val_score(logreg, training_data, target_data, cv=10, scoring='accuracy').mean())\n",
    "\n",
    "svm = svm.SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(clf, training_data, target_data, cv=10, scoring='accuracy')\n",
    "print('SVM cross-validation accuracy: %0.2f' % (cross_val_score(svm, training_data, target_data, cv=10, scoring='accuracy').mean())\n",
    "#####################################################################################################################\n",
    "\n",
    "# 10-Fold Cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "auc = []\n",
    "for train, test in kf.split(X_train, y_train):\n",
    "    pipeline = make_pipeline_imb(SMOTE(), classifier_lr(random_state=42))\n",
    "    model = pipeline.fit(X_train[train], y_train[train])\n",
    "    prediction = model.predict(X_train[test])\n",
    "\n",
    "    accuracy.append(pipeline.score(X_train[test], y_train[test]))\n",
    "    precision.append(precision_score(y_train[test], prediction))\n",
    "    recall.append(recall_score(y_train[test], prediction))\n",
    "    f1.append(f1_score(y_train[test], prediction))\n",
    "    auc.append(roc_auc_score(y_train[test], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Printing mean of several metrics after 5-fold cross validation\n",
    "print(\"Mean of scores 5-fold:\")\n",
    "print(\"Accuracy: {}\".format(np.mean(accuracy)))\n",
    "print(\"Precision: {}\".format(np.mean(precision)))\n",
    "print(\"Recall: {}\".format(np.mean(recall)))\n",
    "print(\"F1: {}\".format(np.mean(f1)))\n",
    "print(\"Auc: {}\".format(np.mean(auc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
